{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Major_Stackgan_stage_2_imp.ipynb","version":"0.3.2","provenance":[{"file_id":"1dFM1fyIlX3MWUHcCD9VhmZ_lx_9Kwy1E","timestamp":1558173244768}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IBQ7TtwcoyZV","colab_type":"code","outputId":"90c27912-bca7-4340-85bb-0a491bbea4e3","executionInfo":{"status":"ok","timestamp":1558757460762,"user_tz":-330,"elapsed":8087,"user":{"displayName":"jitesh pabla","photoUrl":"https://lh3.googleusercontent.com/-ZLjc8KUZpuk/AAAAAAAAAAI/AAAAAAAAADU/kKri8HM-N0o/s64/photo.jpg","userId":"16612763257379949701"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# lfw-created_2 (400 images of our data)\n","!gdown https://drive.google.com/uc?id=1NB1deCJ4XZPvqhTXiRD1AG6FgKHu-h1Y"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1NB1deCJ4XZPvqhTXiRD1AG6FgKHu-h1Y\n","To: /content/lfw-created2.zip\n","\r0.00B [00:00, ?B/s]\r3.63MB [00:00, 114MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nzKSvJnRnrES","colab_type":"code","outputId":"7b39c833-ccf1-46fd-9ef1-484815e2a091","executionInfo":{"status":"ok","timestamp":1558757466813,"user_tz":-330,"elapsed":14091,"user":{"displayName":"jitesh pabla","photoUrl":"https://lh3.googleusercontent.com/-ZLjc8KUZpuk/AAAAAAAAAAI/AAAAAAAAADU/kKri8HM-N0o/s64/photo.jpg","userId":"16612763257379949701"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# faces_em_2 (embeddings of our data of 372)\n","!gdown https://drive.google.com/uc?id=1QrHl3tv7J89S9CyKz4m6_R8F4UXoKHh7"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1QrHl3tv7J89S9CyKz4m6_R8F4UXoKHh7\n","To: /content/faces_em_2.zip\n","15.7MB [00:00, 25.4MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fm0w9KQIYSwF","colab_type":"code","colab":{}},"source":["#! echo '0' > epoch.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_B6MjTRsSFxk","colab_type":"code","outputId":"194b289b-116f-4422-a072-2d3048ac094c","executionInfo":{"status":"ok","timestamp":1558757478800,"user_tz":-330,"elapsed":26016,"user":{"displayName":"jitesh pabla","photoUrl":"https://lh3.googleusercontent.com/-ZLjc8KUZpuk/AAAAAAAAAAI/AAAAAAAAADU/kKri8HM-N0o/s64/photo.jpg","userId":"16612763257379949701"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# stage1_gen\n","!gdown https://drive.google.com/uc?id=1DpaUvhl7nd9BdVBeNEowBL6unQMWhRLg"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1DpaUvhl7nd9BdVBeNEowBL6unQMWhRLg\n","To: /content/stage1_gen.h5\n","41.1MB [00:00, 57.5MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4WsXh38NSFWU","colab_type":"code","outputId":"0519c3ac-37a4-474b-d00b-7504bfe31a44","executionInfo":{"status":"ok","timestamp":1558757488582,"user_tz":-330,"elapsed":35756,"user":{"displayName":"jitesh pabla","photoUrl":"https://lh3.googleusercontent.com/-ZLjc8KUZpuk/AAAAAAAAAAI/AAAAAAAAADU/kKri8HM-N0o/s64/photo.jpg","userId":"16612763257379949701"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# stage2_dis\n","!gdown https://drive.google.com/uc?id=1d3OY4xQX6qGrxw42iqdOU771dHo4DR9o"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1d3OY4xQX6qGrxw42iqdOU771dHo4DR9o\n","To: /content/stage1_dis.h5\n","12.4MB [00:00, 26.7MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZiN8rvfgYTfC","colab_type":"code","colab":{}},"source":["from zipfile import ZipFile\n","\n","\n","with ZipFile(\"lfw-created2.zip\", 'r') as zip2:\n","  zip2.extractall()\n","  zip2.close()\n","  \n","with ZipFile(\"faces_em_2.zip\", 'r') as zip1:\n","  zip1.extractall()\n","  zip1.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9df-gm0FYdtA","colab_type":"code","outputId":"cc5375a4-9e3d-45fa-c20f-4d9b4e6cbbe2","executionInfo":{"status":"ok","timestamp":1558757500893,"user_tz":-330,"elapsed":48024,"user":{"displayName":"jitesh pabla","photoUrl":"https://lh3.googleusercontent.com/-ZLjc8KUZpuk/AAAAAAAAAAI/AAAAAAAAADU/kKri8HM-N0o/s64/photo.jpg","userId":"16612763257379949701"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["!ls\n","!mkdir logs\n","!mkdir results\n","!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["faces_em\tlfw-created2\t  sample_data\t stage1_gen.h5\n","faces_em_2.zip\tlfw-created2.zip  stage1_dis.h5\n","faces_em\tlfw-created2\t  logs\t   sample_data\t  stage1_gen.h5\n","faces_em_2.zip\tlfw-created2.zip  results  stage1_dis.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2frYuh_BIW8J","colab_type":"code","outputId":"f487d9d2-0984-4dcc-cf0e-250619101fcf","executionInfo":{"status":"ok","timestamp":1558757501614,"user_tz":-330,"elapsed":48713,"user":{"displayName":"jitesh pabla","photoUrl":"https://lh3.googleusercontent.com/-ZLjc8KUZpuk/AAAAAAAAAAI/AAAAAAAAADU/kKri8HM-N0o/s64/photo.jpg","userId":"16612763257379949701"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import os\n","import pickle\n","import random\n","import time\n","\n","import PIL\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from PIL import Image\n","from keras import Input, Model\n","from keras import backend as K\n","from keras.callbacks import TensorBoard\n","from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n","    concatenate, Flatten, Lambda, Concatenate, ZeroPadding2D\n","from keras.layers import add\n","from keras.optimizers import Adam\n","from matplotlib import pyplot as plt"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jMbKGBagIg2t","colab_type":"code","colab":{}},"source":["def build_ca_model():\n","    \"\"\"\n","    Get conditioning augmentation model.\n","    Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n","    \"\"\"\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(256)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    model = Model(inputs=[input_layer], outputs=[x])\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YcZBjH3Ijid","colab_type":"code","colab":{}},"source":["def build_embedding_compressor_model():\n","    \"\"\"\n","    Build embedding compressor model\n","    \"\"\"\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(128)(input_layer)\n","    x = ReLU()(x)\n","    model = Model(inputs=[input_layer], outputs=[x])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfA66frZIoat","colab_type":"code","colab":{}},"source":["def generate_c(x):\n","    mean = x[:, :128]\n","    log_sigma = x[:, 128:]\n","\n","    stddev = K.exp(log_sigma)\n","    epsilon = K.random_normal(shape=K.constant((mean.shape[1],), dtype='int32'))\n","    c = stddev * epsilon + mean\n","\n","    return c"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoCkwdM5LEaG","colab_type":"code","colab":{}},"source":["def build_stage1_generator():\n","    \"\"\"\n","    Builds a generator model used in Stage-I\n","    \"\"\"\n","    input_layer = Input(shape=(1024,))\n","    x = Dense(256)(input_layer)\n","    mean_logsigma = LeakyReLU(alpha=0.2)(x)\n","\n","    c = Lambda(generate_c)(mean_logsigma)\n","\n","    input_layer2 = Input(shape=(100,))\n","\n","    gen_input = Concatenate(axis=1)([c, input_layer2])\n","\n","    x = Dense(128 * 8 * 4 * 4, use_bias=False)(gen_input)\n","    x = ReLU()(x)\n","\n","    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = Activation(activation='tanh')(x)\n","\n","    stage1_gen = Model(inputs=[input_layer, input_layer2], outputs=[x, mean_logsigma])\n","    return stage1_gen"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DIIAxxYXIrbU","colab_type":"code","colab":{}},"source":["def residual_block(input):\n","    \"\"\"\n","    Residual block in the generator network\n","    \"\"\"\n","    x = Conv2D(128 * 4, kernel_size=(3, 3), padding='same', strides=1)(input)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(128 * 4, kernel_size=(3, 3), strides=1, padding='same')(x)\n","    x = BatchNormalization()(x)\n","\n","    x = add([x, input])\n","    x = ReLU()(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wADPyw3WIz8d","colab_type":"code","colab":{}},"source":["def joint_block(inputs):\n","    c = inputs[0]\n","    x = inputs[1]\n","\n","    c = K.expand_dims(c, axis=1)\n","    c = K.expand_dims(c, axis=1)\n","    c = K.tile(c, [1, 16, 16, 1])\n","    return K.concatenate([c, x], axis=3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-ETg1zJI2oB","colab_type":"code","colab":{}},"source":["def build_stage2_generator():\n","    \"\"\"\n","    Create Stage-II generator containing the CA Augmentation Network,\n","    the image encoder and the generator network\n","    \"\"\"\n","\n","    # 1. CA Augmentation Network\n","    input_layer = Input(shape=(1024,))\n","    input_lr_images = Input(shape=(64, 64, 3))\n","\n","    ca = Dense(256)(input_layer)\n","    mean_logsigma = LeakyReLU(alpha=0.2)(ca)\n","    c = Lambda(generate_c)(mean_logsigma)\n","\n","    # 2. Image Encoder\n","    x = ZeroPadding2D(padding=(1, 1))(input_lr_images)\n","    x = Conv2D(128, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n","    x = ReLU()(x)\n","\n","    x = ZeroPadding2D(padding=(1, 1))(x)\n","    x = Conv2D(256, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = ZeroPadding2D(padding=(1, 1))(x)\n","    x = Conv2D(512, kernel_size=(4, 4), strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    # 3. Joint\n","    c_code = Lambda(joint_block)([c, x])\n","\n","    x = ZeroPadding2D(padding=(1, 1))(c_code)\n","    x = Conv2D(512, kernel_size=(3, 3), strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    # 4. Residual blocks\n","    x = residual_block(x)\n","    x = residual_block(x)\n","    x = residual_block(x)\n","    x = residual_block(x)\n","\n","    # 5. Upsampling blocks\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = ReLU()(x)\n","\n","    x = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(x)\n","    x = Activation('tanh')(x)\n","\n","    model = Model(inputs=[input_layer, input_lr_images], outputs=[x, mean_logsigma])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fU_WINLI8Kr","colab_type":"code","colab":{}},"source":["def build_stage2_discriminator():\n","    \"\"\"\n","    Create Stage-II discriminator network\n","    \"\"\"\n","    input_layer = Input(shape=(256, 256, 3))\n","\n","    x = Conv2D(64, (4, 4), padding='same', strides=2, input_shape=(256, 256, 3), use_bias=False)(input_layer)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(1024, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(2048, (4, 4), padding='same', strides=2, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(1024, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","\n","    x = Conv2D(512, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","\n","    x2 = Conv2D(128, (1, 1), padding='same', strides=1, use_bias=False)(x)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","\n","    x2 = Conv2D(128, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n","    x2 = BatchNormalization()(x2)\n","    x2 = LeakyReLU(alpha=0.2)(x2)\n","\n","    x2 = Conv2D(512, (3, 3), padding='same', strides=1, use_bias=False)(x2)\n","    x2 = BatchNormalization()(x2)\n","\n","    added_x = add([x, x2])\n","    added_x = LeakyReLU(alpha=0.2)(added_x)\n","\n","    input_layer2 = Input(shape=(4, 4, 128))\n","\n","    merged_input = concatenate([added_x, input_layer2])\n","\n","    x3 = Conv2D(64 * 8, kernel_size=1, padding=\"same\", strides=1)(merged_input)\n","    x3 = BatchNormalization()(x3)\n","    x3 = LeakyReLU(alpha=0.2)(x3)\n","    x3 = Flatten()(x3)\n","    x3 = Dense(1)(x3)\n","    x3 = Activation('sigmoid')(x3)\n","\n","    stage2_dis = Model(inputs=[input_layer, input_layer2], outputs=[x3])\n","    return stage2_dis"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_AUa6BzI_g5","colab_type":"code","colab":{}},"source":["def build_adversarial_model(gen_model2, dis_model, gen_model1):\n","    \"\"\"\n","    Create adversarial model\n","    \"\"\"\n","    embeddings_input_layer = Input(shape=(1024, ))\n","    noise_input_layer = Input(shape=(100, ))\n","    compressed_embedding_input_layer = Input(shape=(4, 4, 128))\n","\n","    gen_model1.trainable = False\n","    dis_model.trainable = False\n","\n","    lr_images, mean_logsigma1 = gen_model1([embeddings_input_layer, noise_input_layer])\n","    hr_images, mean_logsigma2 = gen_model2([embeddings_input_layer, lr_images])\n","    valid = dis_model([hr_images, compressed_embedding_input_layer])\n","\n","    model = Model(inputs=[embeddings_input_layer, noise_input_layer, compressed_embedding_input_layer], outputs=[valid, mean_logsigma2])\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubzRdA4KJBzF","colab_type":"code","colab":{}},"source":["\"\"\"\n","Dataset loading related methods\n","\"\"\"\n","\n","\n","def load_class_ids(class_info_file_path):\n","    \"\"\"\n","    Load class ids from class_info.pickle file\n","    \"\"\"\n","    with open(class_info_file_path, 'rb') as f:\n","        class_ids = pickle.load(f, encoding='latin1')\n","        return class_ids\n","\n","\n","def load_embeddings(embeddings_file_path):\n","    \"\"\"\n","    Function to load embeddings\n","    \"\"\"\n","    with open(embeddings_file_path, 'rb') as f:\n","        embeddings = pickle.load(f, encoding='latin1')\n","        embeddings = np.array(embeddings)\n","        print('embeddings: ', embeddings.shape)\n","    return embeddings\n","\n","\n","def load_filenames(filenames_file_path):\n","    \"\"\"\n","    Load filenames.pickle file and return a list of all file names\n","    \"\"\"\n","    with open(filenames_file_path, 'rb') as f:\n","        filenames = pickle.load(f, encoding='latin1')\n","    return filenames\n","\n","'''\n","def load_bounding_boxes(dataset_dir):\n","    \"\"\"\n","    Load bounding boxes and return a dictionary of file names and corresponding bounding boxes\n","    \"\"\"\n","    # Paths\n","    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n","    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n","\n","    # Read bounding_boxes.txt and images.txt file\n","    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n","                                    delim_whitespace=True, header=None).astype(int)\n","    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n","\n","    # Create a list of file names\n","    file_names = df_file_names[1].tolist()\n","\n","    # Create a dictionary of file_names and bounding boxes\n","    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n","\n","    # Assign a bounding box to the corresponding image\n","    for i in range(0, len(file_names)):\n","        # Get the bounding box\n","        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n","        key = file_names[i][:-4]\n","        filename_boundingbox_dict[key] = bounding_box\n","\n","    return filename_boundingbox_dict\n","'''\n","\n","def get_img(img_path, image_size):\n","    \"\"\"\n","    Load and resize images\n","    \"\"\"\n","    img = Image.open(img_path).convert('RGB')\n","    width, height = img.size\n","    '''\n","    if bbox is not None:\n","        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n","        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n","        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n","        y1 = np.maximum(0, center_y - R)\n","        y2 = np.minimum(height, center_y + R)\n","        x1 = np.maximum(0, center_x - R)\n","        x2 = np.minimum(width, center_x + R)\n","        img = img.crop([x1, y1, x2, y2])\n","    '''\n","    img = img.resize(image_size, PIL.Image.BILINEAR)\n","    return img\n","\n","\n","def load_dataset(filenames_file_path, class_info_file_path, cub_dataset_dir, embeddings_file_path, image_size):\n","    filenames = load_filenames(filenames_file_path)\n","    class_ids = load_class_ids(class_info_file_path)\n","    #bounding_boxes = load_bounding_boxes(cub_dataset_dir)\n","    all_embeddings = load_embeddings(embeddings_file_path)\n","\n","    X, y, embeddings = [], [], []\n","\n","    print(\"All embeddings shape:\", all_embeddings.shape)\n","\n","    for index, filename in enumerate(filenames):\n","        #bounding_box = bounding_boxes[filename]\n","\n","        try:\n","            # Load images\n","            img_name = '{}/{}'.format(cub_dataset_dir, filename)\n","            img = get_img(img_name, image_size)\n","\n","            all_embeddings1 = all_embeddings[index, :, :]\n","\n","            embedding_ix = random.randint(0, all_embeddings1.shape[0] - 1)\n","            embedding = all_embeddings1[embedding_ix, :]\n","\n","            X.append(np.array(img))\n","            y.append(class_ids[index])\n","            embeddings.append(embedding)\n","        except Exception as e:\n","            print(e)\n","\n","    X = np.array(X)\n","    y = np.array(y)\n","    embeddings = np.array(embeddings)\n","\n","    return X, y, embeddings\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYKYJH_DJFIA","colab_type":"code","colab":{}},"source":["\"\"\"\n","Loss functions\n","\"\"\"\n","\n","\n","def KL_loss(y_true, y_pred):\n","    mean = y_pred[:, :128]\n","    logsigma = y_pred[:, :128]\n","    loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mean))\n","    loss = K.mean(loss)\n","    return loss\n","\n","\n","def custom_generator_loss(y_true, y_pred):\n","    # Calculate binary cross entropy loss\n","    return K.binary_crossentropy(y_true, y_pred)\n","\n","\n","def write_log(callback, name, loss, batch_no):\n","    \"\"\"\n","    Write training summary to TensorBoard\n","    \"\"\"\n","    summary = tf.Summary()\n","    summary_value = summary.value.add()\n","    summary_value.simple_value = loss\n","    summary_value.tag = name\n","    callback.writer.add_summary(summary, batch_no)\n","    callback.writer.flush()\n","\n","\n","def save_rgb_img(img, path):\n","    \"\"\"\n","    Save an rgb image\n","    \"\"\"\n","    fig = plt.figure()\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.imshow(img)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Image\")\n","\n","    plt.savefig(path)\n","    plt.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4ApvVY6NjOm","colab_type":"code","outputId":"5dd272f6-0848-40b8-eefc-7a32626d8a3c","executionInfo":{"status":"ok","timestamp":1558757509856,"user_tz":-330,"elapsed":56669,"user":{"displayName":"jitesh pabla","photoUrl":"https://lh3.googleusercontent.com/-ZLjc8KUZpuk/AAAAAAAAAAI/AAAAAAAAADU/kKri8HM-N0o/s64/photo.jpg","userId":"16612763257379949701"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# stage2_gen 6\n","!gdown https://drive.google.com/uc?id=1Tczc0MmiMqOJ70bQl24_IpXd1GFjxHeH"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1Tczc0MmiMqOJ70bQl24_IpXd1GFjxHeH\n","To: /content/stage2_gen.h5\n","115MB [00:02, 56.8MB/s] \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0lmasE1iNu0g","colab_type":"code","outputId":"736e1bb0-3190-4e67-cfdc-105e2c4dc150","executionInfo":{"status":"ok","timestamp":1558757517230,"user_tz":-330,"elapsed":64019,"user":{"displayName":"jitesh pabla","photoUrl":"https://lh3.googleusercontent.com/-ZLjc8KUZpuk/AAAAAAAAAAI/AAAAAAAAADU/kKri8HM-N0o/s64/photo.jpg","userId":"16612763257379949701"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# stage2_dis 6\n","!gdown https://drive.google.com/uc?id=1o99TtXHEYp8eB8Er7I5W18JY8muBY96I"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1o99TtXHEYp8eB8Er7I5W18JY8muBY96I\n","To: /content/stage2_dis.h5\n","194MB [00:03, 64.2MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FG2mWRycCCbi","colab_type":"code","colab":{}},"source":["! echo '358' > epoch.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpqeGU4wJIN3","colab_type":"code","outputId":"e3355be8-6fec-445d-b68a-009c09d568e4","colab":{"base_uri":"https://localhost:8080/","height":39997}},"source":["if __name__ == '__main__':\n","    data_dir = \"/content/faces_em\"\n","    train_dir = data_dir + \"/train\"\n","    test_dir = data_dir + \"/test\"\n","    hr_image_size = (256, 256)\n","    lr_image_size = (64, 64)\n","    batch_size = 8\n","    z_dim = 100\n","    stage1_generator_lr = 0.0002\n","    stage1_discriminator_lr = 0.0002\n","    stage1_lr_decay_step = 600\n","    epochs = 1000\n","    condition_dim = 128\n","\n","    embeddings_file_path_train = train_dir + \"/embed_train.pickle\"\n","    embeddings_file_path_test = test_dir + \"/embed_test.pickle\"\n","\n","    filenames_file_path_train = train_dir + \"/filenames_train.pickle\"\n","    filenames_file_path_test = test_dir + \"/filenames_test.pickle\"\n","\n","    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n","    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n","\n","    cub_dataset_dir = \"/content/lfw-created2\"\n","\n","    # Define optimizers\n","    dis_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","    gen_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n","\n","    \"\"\"\n","    Load datasets\n","    \"\"\"\n","    X_hr_train, y_hr_train, embeddings_train = load_dataset(filenames_file_path=filenames_file_path_train,\n","                                                            class_info_file_path=class_info_file_path_train,\n","                                                            cub_dataset_dir=cub_dataset_dir,\n","                                                            embeddings_file_path=embeddings_file_path_train,\n","                                                            image_size=(256, 256))\n","\n","    X_hr_test, y_hr_test, embeddings_test = load_dataset(filenames_file_path=filenames_file_path_test,\n","                                                         class_info_file_path=class_info_file_path_test,\n","                                                         cub_dataset_dir=cub_dataset_dir,\n","                                                         embeddings_file_path=embeddings_file_path_test,\n","                                                         image_size=(256, 256))\n","\n","    X_lr_train, y_lr_train, _ = load_dataset(filenames_file_path=filenames_file_path_train,\n","                                             class_info_file_path=class_info_file_path_train,\n","                                             cub_dataset_dir=cub_dataset_dir,\n","                                             embeddings_file_path=embeddings_file_path_train,\n","                                             image_size=(64, 64))\n","\n","    X_lr_test, y_lr_test, _ = load_dataset(filenames_file_path=filenames_file_path_test,\n","                                           class_info_file_path=class_info_file_path_test,\n","                                           cub_dataset_dir=cub_dataset_dir,\n","                                           embeddings_file_path=embeddings_file_path_test,\n","                                           image_size=(64, 64))\n","\n","    \"\"\"\n","    Build and compile models\n","    \"\"\"\n","    stage2_dis = build_stage2_discriminator()\n","    stage2_dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n","\n","    stage1_gen = build_stage1_generator()\n","    stage1_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n","\n","    stage1_gen.load_weights(\"stage1_gen.h5\")\n","\n","    stage2_gen = build_stage2_generator()\n","    stage2_gen.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n","\n","    embedding_compressor_model = build_embedding_compressor_model()\n","    embedding_compressor_model.compile(loss='binary_crossentropy', optimizer='adam')\n","\n","    adversarial_model = build_adversarial_model(stage2_gen, stage2_dis, stage1_gen)\n","    adversarial_model.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1.0, 2.0],\n","                              optimizer=gen_optimizer, metrics=None)\n","\n","    tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n","    tensorboard.set_model(stage2_gen)\n","    tensorboard.set_model(stage2_dis)\n","\n","    # Generate an array containing real and fake values\n","    # Apply label smoothing\n","    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n","    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n","\n","    # Resuming the training\n","    file1 = open(\"epoch.txt\",'r') #directory change\n","    epoch_start = 0\n","    epoch_start = int(file1.read())\n","    if(epoch_start!=0):\n","      stage2_gen.load_weights(\"stage2_gen.h5\") #directory change\n","      stage2_dis.load_weights(\"stage2_dis.h5\") #directory change\n","      \n","    for epoch in range(epoch_start, epochs):\n","        print(\"========================================\")\n","        print(\"Epoch is:\", epoch)\n","\n","        gen_losses = []\n","        dis_losses = []\n","\n","        # Load data and train model\n","        number_of_batches = int(X_hr_train.shape[0] / batch_size)\n","        print(\"Number of batches:{}\".format(number_of_batches))\n","        for index in range(number_of_batches):\n","            print(\"Batch:{}\".format(index+1))\n","\n","            # Create a noise vector\n","            z_noise = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            X_hr_train_batch = X_hr_train[index * batch_size:(index + 1) * batch_size]\n","            embedding_batch = embeddings_train[index * batch_size:(index + 1) * batch_size]\n","            X_hr_train_batch = (X_hr_train_batch - 127.5) / 127.5\n","\n","            # Generate fake images\n","            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise], verbose=3)\n","            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n","\n","            \"\"\"\n","            4. Generate compressed embeddings\n","            \"\"\"\n","            compressed_embedding = embedding_compressor_model.predict_on_batch(embedding_batch)\n","            compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, condition_dim))\n","            compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","            \"\"\"\n","            5. Train the discriminator model\n","            \"\"\"\n","            dis_loss_real = stage2_dis.train_on_batch([X_hr_train_batch, compressed_embedding],\n","                                                      np.reshape(real_labels, (batch_size, 1)))\n","            dis_loss_fake = stage2_dis.train_on_batch([hr_fake_images, compressed_embedding],\n","                                                      np.reshape(fake_labels, (batch_size, 1)))\n","            dis_loss_wrong = stage2_dis.train_on_batch([X_hr_train_batch[:(batch_size - 1)], compressed_embedding[1:]],\n","                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n","            d_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong,  dis_loss_fake))\n","            print(\"d_loss:{}\".format(d_loss))\n","\n","            \"\"\"\n","            Train the adversarial model\n","            \"\"\"\n","            g_loss = adversarial_model.train_on_batch([embedding_batch, z_noise, compressed_embedding],\n","                                                                [K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n","\n","            print(\"g_loss:{}\".format(g_loss))\n","\n","            dis_losses.append(d_loss)\n","            gen_losses.append(g_loss)\n","\n","        \"\"\"\n","        Save losses to Tensorboard after each epoch\n","        \"\"\"\n","        write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n","        #write_log(tensorboard, 'generator_loss', np.mean(gen_losses)[0], epoch)\n","        write_log(tensorboard, 'generator_loss', np.mean(gen_losses), epoch)\n","\n","        # Generate and save images after every 2nd epoch\n","        if epoch % 2 == 0:\n","            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n","            z_noise2 = np.random.normal(0, 1, size=(batch_size, z_dim))\n","            embedding_batch = embeddings_test[0:batch_size]\n","\n","            lr_fake_images, _ = stage1_gen.predict([embedding_batch, z_noise2], verbose=3)\n","            hr_fake_images, _ = stage2_gen.predict([embedding_batch, lr_fake_images], verbose=3)\n","\n","            # saving weights as a checkpoint\n","            stage2_gen.save_weights(\"stage2_gen.h5\") #directory change\n","            stage2_dis.save_weights(\"stage2_dis.h5\") #directory change\n","            file = open(\"epoch.txt\",'w') #directory change\n","            file.write(str(epoch))\n","            \n","            # Save images\n","            for i, img in enumerate(hr_fake_images[:10]):\n","                save_rgb_img(img, \"results/gen_{}_{}.png\".format(epoch, i))\n","\n","        if epoch % 4 == 0:\n","          \n","            import shutil\n","            shutil.make_archive(\"results\", 'zip', \"results\")\n","            shutil.make_archive(\"logs\", 'zip', \"logs\")\n","            \n","    # Saving the models\n","    stage2_gen.save_weights(\"stage2_gen.h5\")\n","    stage2_dis.save_weights(\"stage2_dis.h5\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","embeddings:  (300, 1, 1024)\n","All embeddings shape: (300, 1, 1024)\n","embeddings:  (73, 1, 1024)\n","All embeddings shape: (73, 1, 1024)\n","embeddings:  (300, 1, 1024)\n","All embeddings shape: (300, 1, 1024)\n","embeddings:  (73, 1, 1024)\n","All embeddings shape: (73, 1, 1024)\n","========================================\n","Epoch is: 358\n","Number of batches:37\n","Batch:1\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["d_loss:0.16690214298432693\n","g_loss:[0.39230898, 0.3922538, 2.7596794e-05]\n","Batch:2\n","d_loss:0.16489921542233787\n","g_loss:[0.9033159, 0.90326905, 2.3410003e-05]\n","Batch:3\n","d_loss:0.28628901904448867\n","g_loss:[0.39628783, 0.396243, 2.2416605e-05]\n","Batch:4\n","d_loss:0.463080495595932\n","g_loss:[2.9876335, 2.9875913, 2.1136368e-05]\n","Batch:5\n","d_loss:0.18506181798875332\n","g_loss:[0.3647068, 0.3646589, 2.3956043e-05]\n","Batch:6\n","d_loss:0.18017483176663518\n","g_loss:[0.59029454, 0.5902454, 2.4556723e-05]\n","Batch:7\n","d_loss:0.1707058663596399\n","g_loss:[0.5099668, 0.50990146, 3.2677577e-05]\n","Batch:8\n","d_loss:0.16675298305926844\n","g_loss:[0.33648306, 0.33638388, 4.959319e-05]\n","Batch:9\n","d_loss:0.16887802464771084\n","g_loss:[0.34086463, 0.34077358, 4.5519922e-05]\n","Batch:10\n","d_loss:0.16444732126547024\n","g_loss:[1.5215632, 1.521497, 3.3058495e-05]\n","Batch:11\n","d_loss:0.16528231388656422\n","g_loss:[0.3471945, 0.34715888, 1.7802498e-05]\n","Batch:12\n","d_loss:0.16933421092107892\n","g_loss:[0.32894492, 0.32891634, 1.4287459e-05]\n","Batch:13\n","d_loss:0.1665554873761721\n","g_loss:[0.32942936, 0.3294044, 1.24795115e-05]\n","Batch:14\n","d_loss:0.16671787851373665\n","g_loss:[0.47298646, 0.4729641, 1.11772215e-05]\n","Batch:15\n","d_loss:0.16394685160776135\n","g_loss:[0.33793005, 0.337906, 1.2029279e-05]\n","Batch:16\n","d_loss:0.4010932594537735\n","g_loss:[0.32997742, 0.329956, 1.0709578e-05]\n","Batch:17\n","d_loss:0.16519660809717607\n","g_loss:[0.36051327, 0.36049563, 8.822724e-06]\n","Batch:18\n","d_loss:0.2533195726573467\n","g_loss:[0.3671218, 0.3671061, 7.835946e-06]\n","Batch:19\n","d_loss:0.20739638000668492\n","g_loss:[0.80160654, 0.8015921, 7.206003e-06]\n","Batch:20\n","d_loss:0.1655095814494416\n","g_loss:[0.35340753, 0.3533925, 7.512026e-06]\n","Batch:21\n","d_loss:0.1648248079000041\n","g_loss:[0.36725438, 0.36724073, 6.830824e-06]\n","Batch:22\n","d_loss:0.16582030509016477\n","g_loss:[0.35817832, 0.35816634, 5.9932945e-06]\n","Batch:23\n","d_loss:0.16795003848528722\n","g_loss:[0.4154401, 0.41542956, 5.2765613e-06]\n","Batch:24\n","d_loss:0.17414928661310114\n","g_loss:[0.73449093, 0.7344813, 4.8288725e-06]\n","Batch:25\n","d_loss:0.17713332967832685\n","g_loss:[0.33051622, 0.3305031, 6.551165e-06]\n","Batch:26\n","d_loss:0.20400334795704111\n","g_loss:[0.38721052, 0.38720042, 5.0441927e-06]\n","Batch:27\n","d_loss:0.1784771685488522\n","g_loss:[0.36255, 0.36254197, 4.010462e-06]\n","Batch:28\n","d_loss:0.16548263464937918\n","g_loss:[0.3889225, 0.38891512, 3.701685e-06]\n","Batch:29\n","d_loss:0.1689829109236598\n","g_loss:[0.42949966, 0.4294926, 3.5350056e-06]\n","Batch:30\n","d_loss:0.17016148650145624\n","g_loss:[0.37671608, 0.37671, 3.0355534e-06]\n","Batch:31\n","d_loss:0.17340980120934546\n","g_loss:[0.36324787, 0.36324248, 2.6996197e-06]\n","Batch:32\n","d_loss:0.1753052962012589\n","g_loss:[0.34678015, 0.3467753, 2.4250207e-06]\n","Batch:33\n","d_loss:0.16366349610325415\n","g_loss:[2.5515366, 2.5515323, 2.1894357e-06]\n","Batch:34\n","d_loss:0.1712915107200388\n","g_loss:[0.81453025, 0.8145152, 7.5468397e-06]\n","Batch:35\n","d_loss:0.1768359420238994\n","g_loss:[0.48716563, 0.48713326, 1.6186492e-05]\n","Batch:36\n","d_loss:0.16440927882831602\n","g_loss:[0.36395967, 0.36393481, 1.2424576e-05]\n","Batch:37\n","d_loss:0.17116474916110747\n","g_loss:[0.4203932, 0.4203723, 1.04458095e-05]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 359\n","Number of batches:37\n","Batch:1\n","d_loss:0.16428913525305688\n","g_loss:[0.37904632, 0.37902564, 1.0334007e-05]\n","Batch:2\n","d_loss:0.1750691650668159\n","g_loss:[0.5371888, 0.5371698, 9.5016985e-06]\n","Batch:3\n","d_loss:0.17679546517319977\n","g_loss:[0.34278753, 0.3427669, 1.0310239e-05]\n","Batch:4\n","d_loss:0.7428201138973236\n","g_loss:[0.36628395, 0.3662638, 1.0076852e-05]\n","Batch:5\n","d_loss:0.17559250281192362\n","g_loss:[0.44354182, 0.4435251, 8.3568775e-06]\n","Batch:6\n","d_loss:0.16706668032566085\n","g_loss:[0.38558036, 0.38556623, 7.056643e-06]\n","Batch:7\n","d_loss:0.17112430441193283\n","g_loss:[0.38412604, 0.3841135, 6.277922e-06]\n","Batch:8\n","d_loss:0.1663431156775914\n","g_loss:[3.2315216, 3.2315102, 5.702591e-06]\n","Batch:9\n","d_loss:0.16489652528252918\n","g_loss:[0.3439087, 0.34388974, 9.47957e-06]\n","Batch:10\n","d_loss:0.16762544424273074\n","g_loss:[1.186135, 1.1861229, 6.099726e-06]\n","Batch:11\n","d_loss:0.16850861048442312\n","g_loss:[0.7856158, 0.78558004, 1.789334e-05]\n","Batch:12\n","d_loss:0.16574821341782808\n","g_loss:[1.7327244, 1.7327034, 1.0515991e-05]\n","Batch:13\n","d_loss:0.16894678931566887\n","g_loss:[0.5460654, 0.5460342, 1.5588124e-05]\n","Batch:14\n","d_loss:0.1646302264998667\n","g_loss:[0.39142483, 0.39139223, 1.6303342e-05]\n","Batch:15\n","d_loss:0.16383162159763742\n","g_loss:[0.39060983, 0.39058968, 1.0070826e-05]\n","Batch:16\n","d_loss:0.5491960942745209\n","g_loss:[0.38200793, 0.38199577, 6.085428e-06]\n","Batch:17\n","d_loss:0.1680967346765101\n","g_loss:[0.35766435, 0.35765323, 5.559989e-06]\n","Batch:18\n","d_loss:0.2418495500460267\n","g_loss:[0.3490751, 0.34906483, 5.145409e-06]\n","Batch:19\n","d_loss:0.17040686486870982\n","g_loss:[2.4317908, 2.4317813, 4.81962e-06]\n","Batch:20\n","d_loss:0.16874305706005543\n","g_loss:[0.43510255, 0.43509334, 4.599041e-06]\n","Batch:21\n","d_loss:0.16503968290635385\n","g_loss:[0.52987, 0.5298606, 4.6913015e-06]\n","Batch:22\n","d_loss:0.16368305130163208\n","g_loss:[2.4856226, 2.4855778, 2.2465021e-05]\n","Batch:23\n","d_loss:0.16442028261371888\n","g_loss:[0.34836045, 0.3482463, 5.7077545e-05]\n","Batch:24\n","d_loss:0.169898989261128\n","g_loss:[0.38045293, 0.38037845, 3.7242648e-05]\n","Batch:25\n","d_loss:0.17694158758968115\n","g_loss:[0.37449935, 0.3744694, 1.4971364e-05]\n","Batch:26\n","d_loss:0.17462073626666097\n","g_loss:[0.37329766, 0.37328166, 8.003088e-06]\n","Batch:27\n","d_loss:0.1715783411054872\n","g_loss:[0.3595557, 0.35954267, 6.5131294e-06]\n","Batch:28\n","d_loss:0.1754573475336656\n","g_loss:[0.3819287, 0.38191658, 6.0679554e-06]\n","Batch:29\n","d_loss:0.16909187974306406\n","g_loss:[0.4187373, 0.4187261, 5.605763e-06]\n","Batch:30\n","d_loss:0.16872480977326632\n","g_loss:[0.40210938, 0.40209472, 7.330502e-06]\n","Batch:31\n","d_loss:0.162973094782501\n","g_loss:[0.40717837, 0.4071628, 7.798211e-06]\n","Batch:32\n","d_loss:0.1653244299814105\n","g_loss:[0.35448554, 0.3544719, 6.819386e-06]\n","Batch:33\n","d_loss:0.19032624084502459\n","g_loss:[0.39016828, 0.39015663, 5.8189667e-06]\n","Batch:34\n","d_loss:0.1737724960257765\n","g_loss:[0.41048887, 0.4104792, 4.8383977e-06]\n","Batch:35\n","d_loss:0.1669943251181394\n","g_loss:[0.3346386, 0.3346305, 4.0543223e-06]\n","Batch:36\n","d_loss:0.2027115449309349\n","g_loss:[0.39950517, 0.39949805, 3.5610296e-06]\n","Batch:37\n","d_loss:0.1843825330142863\n","g_loss:[0.33521366, 0.3352067, 3.4804634e-06]\n","========================================\n","Epoch is: 360\n","Number of batches:37\n","Batch:1\n","d_loss:0.18074813656858169\n","g_loss:[1.24736, 1.2473533, 3.3516321e-06]\n","Batch:2\n","d_loss:0.17474771144043189\n","g_loss:[0.3596517, 0.3595131, 6.929944e-05]\n","Batch:3\n","d_loss:0.1906232964247465\n","g_loss:[0.34196627, 0.34174657, 0.00010985125]\n","Batch:4\n","d_loss:0.6575217396020889\n","g_loss:[0.33241785, 0.3322826, 6.7626235e-05]\n","Batch:5\n","d_loss:0.1879402818158269\n","g_loss:[0.36499226, 0.36492252, 3.486562e-05]\n","Batch:6\n","d_loss:0.1922797498991713\n","g_loss:[0.32941628, 0.32937086, 2.2715109e-05]\n","Batch:7\n","d_loss:0.17098359134979546\n","g_loss:[0.33899257, 0.33895332, 1.9621451e-05]\n","Batch:8\n","d_loss:0.16847766912542284\n","g_loss:[0.6033461, 0.6033095, 1.8309214e-05]\n","Batch:9\n","d_loss:0.17763463539085933\n","g_loss:[0.44486776, 0.44482374, 2.2002227e-05]\n","Batch:10\n","d_loss:0.172447067219764\n","g_loss:[0.3653173, 0.36526692, 2.5193305e-05]\n","Batch:11\n","d_loss:0.1638622512400616\n","g_loss:[0.34673697, 0.34669012, 2.3431652e-05]\n","Batch:12\n","d_loss:0.1787485409877263\n","g_loss:[0.33452305, 0.33448344, 1.980284e-05]\n","Batch:13\n","d_loss:0.17020791035611182\n","g_loss:[1.9942852, 1.9942518, 1.668964e-05]\n","Batch:14\n","d_loss:0.18176306194072822\n","g_loss:[1.7000865, 1.699991, 4.77219e-05]\n","Batch:15\n","d_loss:0.17408788751345128\n","g_loss:[0.35209954, 0.35194752, 7.601622e-05]\n","Batch:16\n","d_loss:0.3235943019390106\n","g_loss:[0.35238528, 0.35221845, 8.3413026e-05]\n","Batch:17\n","d_loss:0.17270926958008204\n","g_loss:[0.33513463, 0.33499545, 6.958163e-05]\n","Batch:18\n","d_loss:0.247195303440094\n","g_loss:[4.7252088, 4.72509, 5.9400736e-05]\n","Batch:19\n","d_loss:0.20365291647613049\n","g_loss:[1.2255471, 1.2251358, 0.00020561443]\n","Batch:20\n","d_loss:0.23702892288565636\n","g_loss:[0.42219543, 0.42178926, 0.00020308279]\n","Batch:21\n","d_loss:0.2770330607891083\n","g_loss:[0.33391112, 0.33346575, 0.00022267706]\n","Batch:22\n","d_loss:0.22325093961990206\n","g_loss:[0.3747838, 0.37435138, 0.00021621544]\n","Batch:23\n","d_loss:0.2584497854113579\n","g_loss:[1.377305, 1.3769084, 0.00019828504]\n","Batch:24\n","d_loss:0.18289775447919965\n","g_loss:[1.0929608, 1.0925779, 0.00019146915]\n","Batch:25\n","d_loss:0.335182698443532\n","g_loss:[1.205082, 1.2047164, 0.00018282796]\n","Batch:26\n","d_loss:0.1931388398515992\n","g_loss:[0.5389802, 0.53862965, 0.00017525489]\n","Batch:27\n","d_loss:0.17862756381873623\n","g_loss:[2.1151896, 2.1148648, 0.00016233341]\n","Batch:28\n","d_loss:0.16725374216184719\n","g_loss:[0.33124352, 0.3309453, 0.0001490962]\n","Batch:29\n","d_loss:0.17610867190524004\n","g_loss:[0.32924095, 0.32896927, 0.00013583599]\n","Batch:30\n","d_loss:0.20134857529774308\n","g_loss:[0.38586617, 0.38561323, 0.00012646907]\n","Batch:31\n","d_loss:0.18847726500825956\n","g_loss:[0.55468595, 0.554446, 0.000119991186]\n","Batch:32\n","d_loss:0.17114406573818997\n","g_loss:[0.5878076, 0.5875767, 0.00011546353]\n","Batch:33\n","d_loss:0.1872464930638671\n","g_loss:[1.5699241, 1.5697007, 0.00011169141]\n","Batch:34\n","d_loss:0.17115515455952846\n","g_loss:[0.56581736, 0.5655828, 0.000117282965]\n","Batch:35\n","d_loss:0.1666610988031607\n","g_loss:[0.36299127, 0.36274198, 0.00012465469]\n","Batch:36\n","d_loss:0.1646192395746766\n","g_loss:[0.3894461, 0.38919944, 0.00012333231]\n","Batch:37\n","d_loss:0.17832750314846635\n","g_loss:[0.3946953, 0.39446428, 0.000115515664]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 361\n","Number of batches:37\n","Batch:1\n","d_loss:0.1712478060508147\n","g_loss:[0.3942367, 0.39401573, 0.00011049841]\n","Batch:2\n","d_loss:0.16489135660231113\n","g_loss:[0.41242802, 0.4122141, 0.00010696412]\n","Batch:3\n","d_loss:0.16383568433229811\n","g_loss:[0.3956641, 0.39545718, 0.00010345699]\n","Batch:4\n","d_loss:0.5611960887908936\n","g_loss:[0.35281426, 0.35261416, 0.00010004475]\n","Batch:5\n","d_loss:0.16450932635052595\n","g_loss:[0.38034663, 0.38015312, 9.6758864e-05]\n","Batch:6\n","d_loss:0.17739890981465578\n","g_loss:[1.6605104, 1.6603225, 9.393359e-05]\n","Batch:7\n","d_loss:0.1703534775879234\n","g_loss:[0.3494528, 0.34926963, 9.158843e-05]\n","Batch:8\n","d_loss:0.17936442006612197\n","g_loss:[2.2971237, 2.2969453, 8.919567e-05]\n","Batch:9\n","d_loss:0.16687792031552817\n","g_loss:[0.3880892, 0.3878988, 9.520725e-05]\n","Batch:10\n","d_loss:0.17273831355851144\n","g_loss:[0.36764807, 0.36745316, 9.745105e-05]\n","Batch:11\n","d_loss:0.1663122421305161\n","g_loss:[2.0557108, 2.0555182, 9.633919e-05]\n","Batch:12\n","d_loss:0.17631741933291778\n","g_loss:[0.39029872, 0.39010924, 9.4746894e-05]\n","Batch:13\n","d_loss:0.18281863300944678\n","g_loss:[2.2650597, 2.2648811, 8.9251575e-05]\n","Batch:14\n","d_loss:0.16477368042978924\n","g_loss:[0.33884388, 0.33867952, 8.2182116e-05]\n","Batch:15\n","d_loss:0.1701146569357661\n","g_loss:[0.36754772, 0.36739314, 7.729183e-05]\n","Batch:16\n","d_loss:0.7437822818756104\n","g_loss:[0.35934454, 0.35919607, 7.423727e-05]\n","Batch:17\n","d_loss:0.28571295738220215\n","g_loss:[1.2098264, 1.2096825, 7.193562e-05]\n","Batch:18\n","d_loss:0.6107541769742966\n","g_loss:[1.6653221, 1.66517, 7.602862e-05]\n","Batch:19\n","d_loss:0.26248048432171345\n","g_loss:[1.2059758, 1.2057546, 0.00011055634]\n","Batch:20\n","d_loss:0.2414592822897248\n","g_loss:[0.72777176, 0.7274916, 0.00014006541]\n","Batch:21\n","d_loss:0.47026941180229187\n","g_loss:[0.77738154, 0.77709717, 0.00014218932]\n","Batch:22\n","d_loss:0.4109241832047701\n","g_loss:[3.3889132, 3.3886504, 0.00013133803]\n","Batch:23\n","d_loss:0.2036203402094543\n","g_loss:[4.0395036, 4.039132, 0.00018584442]\n","Batch:24\n","d_loss:0.17519470897968858\n","g_loss:[0.73198926, 0.7315868, 0.00020121907]\n","Batch:25\n","d_loss:0.21530423464719206\n","g_loss:[0.683082, 0.68272746, 0.0001772652]\n","Batch:26\n","d_loss:0.18173213361296803\n","g_loss:[0.49865818, 0.4983468, 0.00015569048]\n","Batch:27\n","d_loss:0.20963642746210098\n","g_loss:[1.8205776, 1.8203025, 0.00013758588]\n","Batch:28\n","d_loss:0.17028019900317304\n","g_loss:[0.4305729, 0.43027985, 0.00014652315]\n","Batch:29\n","d_loss:0.16728902063914575\n","g_loss:[0.40495345, 0.404656, 0.00014873149]\n","Batch:30\n","d_loss:0.43977056443691254\n","g_loss:[0.5270842, 0.52680093, 0.00014161474]\n","Batch:31\n","d_loss:0.17288555763661861\n","g_loss:[0.78017306, 0.77991426, 0.00012940339]\n","Batch:32\n","d_loss:0.7378936260938644\n","g_loss:[0.63708264, 0.636848, 0.00011732319]\n","Batch:33\n","d_loss:0.17333383485674858\n","g_loss:[0.46009392, 0.4598792, 0.00010736352]\n","Batch:34\n","d_loss:0.20010689552873373\n","g_loss:[0.4990514, 0.498851, 0.00010019268]\n","Batch:35\n","d_loss:0.19774358347058296\n","g_loss:[0.43469408, 0.4345061, 9.399532e-05]\n","Batch:36\n","d_loss:0.17893464118242264\n","g_loss:[0.39063114, 0.39045277, 8.9179695e-05]\n","Batch:37\n","d_loss:0.18488306523067877\n","g_loss:[0.5445894, 0.54441893, 8.523654e-05]\n","========================================\n","Epoch is: 362\n","Number of batches:37\n","Batch:1\n","d_loss:0.17208979884162545\n","g_loss:[0.39192337, 0.39175808, 8.26464e-05]\n","Batch:2\n","d_loss:0.16901859536301345\n","g_loss:[0.42135775, 0.42119756, 8.009829e-05]\n","Batch:3\n","d_loss:0.1744367553619668\n","g_loss:[0.39744276, 0.3972876, 7.757382e-05]\n","Batch:4\n","d_loss:0.5030591040849686\n","g_loss:[0.38257566, 0.38242498, 7.534679e-05]\n","Batch:5\n","d_loss:0.18297570827417076\n","g_loss:[0.3962897, 0.39614323, 7.3244126e-05]\n","Batch:6\n","d_loss:0.1754414026509039\n","g_loss:[2.1945806, 2.194438, 7.128794e-05]\n","Batch:7\n","d_loss:0.17232560424599797\n","g_loss:[0.43944263, 0.4392764, 8.311732e-05]\n","Batch:8\n","d_loss:0.17690446181222796\n","g_loss:[0.5865207, 0.5863294, 9.5642215e-05]\n","Batch:9\n","d_loss:0.168762509842054\n","g_loss:[0.43038622, 0.4301875, 9.936155e-05]\n","Batch:10\n","d_loss:0.172546973451972\n","g_loss:[0.49901626, 0.49881893, 9.865918e-05]\n","Batch:11\n","d_loss:0.16951439625700004\n","g_loss:[0.3906268, 0.39043796, 9.441834e-05]\n","Batch:12\n","d_loss:0.1727784447139129\n","g_loss:[0.37197527, 0.37179607, 8.960074e-05]\n","Batch:13\n","d_loss:0.17979752662358806\n","g_loss:[0.79097337, 0.79080355, 8.49209e-05]\n","Batch:14\n","d_loss:0.1650371136056492\n","g_loss:[0.35025817, 0.35009885, 7.966255e-05]\n","Batch:15\n","d_loss:0.16658257546077948\n","g_loss:[0.34427464, 0.34412414, 7.524433e-05]\n","Batch:16\n","d_loss:0.2633482627570629\n","g_loss:[0.35200578, 0.3518629, 7.1433846e-05]\n","Batch:17\n","d_loss:0.16638185893498303\n","g_loss:[0.36397094, 0.36383462, 6.8152585e-05]\n","Batch:18\n","d_loss:0.1781126477289945\n","g_loss:[0.34665337, 0.34652287, 6.525329e-05]\n","Batch:19\n","d_loss:0.18220724206184968\n","g_loss:[0.3332809, 0.33315566, 6.261695e-05]\n","Batch:20\n","d_loss:0.18956652912311256\n","g_loss:[0.3731112, 0.37299073, 6.0234473e-05]\n","Batch:21\n","d_loss:0.1970541258342564\n","g_loss:[0.39394966, 0.39383358, 5.8039575e-05]\n","Batch:22\n","d_loss:0.18760349701915402\n","g_loss:[0.46475697, 0.46464473, 5.6121928e-05]\n","Batch:23\n","d_loss:0.16540602646273328\n","g_loss:[0.46832582, 0.4682173, 5.425057e-05]\n","Batch:24\n","d_loss:0.1680003801593557\n","g_loss:[0.37175843, 0.37165272, 5.285296e-05]\n","Batch:25\n","d_loss:0.16537697698367992\n","g_loss:[0.37466204, 0.37455937, 5.132836e-05]\n","Batch:26\n","d_loss:0.16484566330109374\n","g_loss:[0.37658176, 0.37648198, 4.989117e-05]\n","Batch:27\n","d_loss:0.16806949704914587\n","g_loss:[0.34887618, 0.3487791, 4.8537364e-05]\n","Batch:28\n","d_loss:0.16648535049171187\n","g_loss:[0.36765996, 0.36756527, 4.733537e-05]\n","Batch:29\n","d_loss:0.16537003971461672\n","g_loss:[0.34458968, 0.34449753, 4.6080404e-05]\n","Batch:30\n","d_loss:0.18010370196134318\n","g_loss:[0.39704236, 0.3969524, 4.4980072e-05]\n","Batch:31\n","d_loss:0.1743272906205675\n","g_loss:[0.38673192, 0.38664395, 4.3988417e-05]\n","Batch:32\n","d_loss:0.1673202504607616\n","g_loss:[2.9224415, 2.9223554, 4.3030104e-05]\n","Batch:33\n","d_loss:0.1728483588231029\n","g_loss:[0.4332788, 0.43319666, 4.106315e-05]\n","Batch:34\n","d_loss:0.17136229243988055\n","g_loss:[0.37207603, 0.37199637, 3.983028e-05]\n","Batch:35\n","d_loss:0.18931373197119683\n","g_loss:[0.34683818, 0.34676045, 3.885516e-05]\n","Batch:36\n","d_loss:0.16586371985977166\n","g_loss:[0.35284185, 0.35276592, 3.7966307e-05]\n","Batch:37\n","d_loss:0.1758317316416651\n","g_loss:[0.3406945, 0.3406201, 3.7197977e-05]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 363\n","Number of batches:37\n","Batch:1\n","d_loss:0.16506440707598813\n","g_loss:[0.3874404, 0.3873675, 3.646099e-05]\n","Batch:2\n","d_loss:0.16533103340043453\n","g_loss:[0.3343816, 0.33430964, 3.599142e-05]\n","Batch:3\n","d_loss:0.16451244079507887\n","g_loss:[0.3525255, 0.35245454, 3.548703e-05]\n","Batch:4\n","d_loss:0.4655824452638626\n","g_loss:[0.34283406, 0.3427642, 3.4923687e-05]\n","Batch:5\n","d_loss:0.16984194668475538\n","g_loss:[0.41275594, 0.41268718, 3.437587e-05]\n","Batch:6\n","d_loss:0.16801231789577287\n","g_loss:[0.44423848, 0.4441709, 3.379998e-05]\n","Batch:7\n","d_loss:0.16714832917205058\n","g_loss:[0.43335462, 0.4332882, 3.3217628e-05]\n","Batch:8\n","d_loss:0.16538195907196496\n","g_loss:[0.43263936, 0.432574, 3.2680335e-05]\n","Batch:9\n","d_loss:0.17425962167180842\n","g_loss:[0.33808887, 0.33802482, 3.2016793e-05]\n","Batch:10\n","d_loss:0.16799646746949293\n","g_loss:[0.43496525, 0.43490237, 3.144233e-05]\n","Batch:11\n","d_loss:0.16339198776404373\n","g_loss:[0.34772474, 0.3476631, 3.081964e-05]\n","Batch:12\n","d_loss:0.168012012320105\n","g_loss:[0.3668112, 0.36675072, 3.0230007e-05]\n","Batch:13\n","d_loss:0.16835659529897384\n","g_loss:[0.35011017, 0.35005084, 2.9662822e-05]\n","Batch:14\n","d_loss:0.16345623562665423\n","g_loss:[0.34603193, 0.3459736, 2.915524e-05]\n","Batch:15\n","d_loss:0.16439918109972496\n","g_loss:[0.35491896, 0.35486162, 2.8669316e-05]\n","Batch:16\n","d_loss:0.2738689072430134\n","g_loss:[0.3565683, 0.35651183, 2.8239032e-05]\n","Batch:17\n","d_loss:0.16740475138067268\n","g_loss:[0.34900206, 0.34894642, 2.7817623e-05]\n","Batch:18\n","d_loss:0.17401456006336957\n","g_loss:[0.33151108, 0.3314563, 2.7384633e-05]\n","Batch:19\n","d_loss:0.180273065547226\n","g_loss:[0.33479083, 0.33473694, 2.6946207e-05]\n","Batch:20\n","d_loss:0.19425747531931847\n","g_loss:[0.33615783, 0.33610475, 2.6535685e-05]\n","Batch:21\n","d_loss:0.16946107320836745\n","g_loss:[0.3682786, 0.3682263, 2.6156433e-05]\n","Batch:22\n","d_loss:0.1790919598643086\n","g_loss:[1.850731, 1.8506709, 3.0038744e-05]\n","Batch:23\n","d_loss:0.1646059380582301\n","g_loss:[0.35121307, 0.35115188, 3.059469e-05]\n","Batch:24\n","d_loss:0.163407302898122\n","g_loss:[0.342297, 0.34224284, 2.7069784e-05]\n","Batch:25\n","d_loss:0.1680741456511896\n","g_loss:[0.35983744, 0.35978884, 2.4299947e-05]\n","Batch:26\n","d_loss:0.16649938162663602\n","g_loss:[0.3603032, 0.3602562, 2.3503471e-05]\n","Batch:27\n","d_loss:0.16572753561194986\n","g_loss:[0.39951187, 0.39946574, 2.3069615e-05]\n","Batch:28\n","d_loss:0.16402944338187808\n","g_loss:[0.38681707, 0.38677138, 2.2838947e-05]\n","Batch:29\n","d_loss:0.1638670350293978\n","g_loss:[0.3479328, 0.34788764, 2.2579163e-05]\n","Batch:30\n","d_loss:0.1663905132227228\n","g_loss:[0.3935926, 0.39354804, 2.2276077e-05]\n","Batch:31\n","d_loss:0.16320761647511972\n","g_loss:[0.3832215, 0.3831777, 2.1909764e-05]\n","Batch:32\n","d_loss:0.1639837605325738\n","g_loss:[0.34760082, 0.3475576, 2.16072e-05]\n","Batch:33\n","d_loss:0.1649417395638011\n","g_loss:[0.3650198, 0.3649772, 2.1293772e-05]\n","Batch:34\n","d_loss:0.16829018155840458\n","g_loss:[0.40672794, 0.4066859, 2.1023112e-05]\n","Batch:35\n","d_loss:0.16446749954775441\n","g_loss:[0.33015117, 0.33010963, 2.0770547e-05]\n","Batch:36\n","d_loss:0.16956194322665397\n","g_loss:[0.34522253, 0.34518158, 2.048101e-05]\n","Batch:37\n","d_loss:0.16894532363221515\n","g_loss:[0.32952294, 0.32948261, 2.0164607e-05]\n","========================================\n","Epoch is: 364\n","Number of batches:37\n","Batch:1\n","d_loss:0.16482690940392786\n","g_loss:[0.32722616, 0.32718647, 1.9854413e-05]\n","Batch:2\n","d_loss:0.16659766216980643\n","g_loss:[0.34447318, 0.3444341, 1.953564e-05]\n","Batch:3\n","d_loss:0.16768677520303754\n","g_loss:[0.32834435, 0.32830584, 1.9258088e-05]\n","Batch:4\n","d_loss:0.4755949229001999\n","g_loss:[0.34318852, 0.34315062, 1.8960736e-05]\n","Batch:5\n","d_loss:0.17283250019681873\n","g_loss:[2.1334887, 2.1334512, 1.869181e-05]\n","Batch:6\n","d_loss:0.16641394782345742\n","g_loss:[0.3872214, 0.3871831, 1.9147565e-05]\n","Batch:7\n","d_loss:0.16333709591708612\n","g_loss:[0.4059304, 0.4058932, 1.8603274e-05]\n","Batch:8\n","d_loss:0.1632888171589002\n","g_loss:[0.35503924, 0.3550037, 1.7767938e-05]\n","Batch:9\n","d_loss:0.17628722934387042\n","g_loss:[0.3312594, 0.33122507, 1.7163607e-05]\n","Batch:10\n","d_loss:0.16558830652502365\n","g_loss:[1.9121798, 1.9121463, 1.6760556e-05]\n","Batch:11\n","d_loss:0.16515935478673782\n","g_loss:[0.33307207, 0.33299536, 3.834941e-05]\n","Batch:12\n","d_loss:0.16825408552540466\n","g_loss:[0.3496525, 0.34953028, 6.110225e-05]\n","Batch:13\n","d_loss:0.16334987778100185\n","g_loss:[0.38593242, 0.38579115, 7.062516e-05]\n","Batch:14\n","d_loss:0.16304916561057325\n","g_loss:[0.3487566, 0.34860903, 7.378725e-05]\n","Batch:15\n","d_loss:0.16913760235183872\n","g_loss:[0.363233, 0.36308283, 7.509346e-05]\n","Batch:16\n","d_loss:0.2445227922871709\n","g_loss:[0.35168192, 0.351533, 7.44645e-05]\n","Batch:17\n","d_loss:0.1642474670697993\n","g_loss:[0.3392415, 0.33909628, 7.261644e-05]\n","Batch:18\n","d_loss:0.16566442023031414\n","g_loss:[0.41200617, 0.41186556, 7.03053e-05]\n","Batch:19\n","d_loss:0.17067462359045749\n","g_loss:[0.35227838, 0.35214275, 6.781336e-05]\n","Batch:20\n","d_loss:0.18481033068383113\n","g_loss:[0.35152087, 0.3513903, 6.528916e-05]\n","Batch:21\n","d_loss:0.16413831594400108\n","g_loss:[0.33288473, 0.3327592, 6.276975e-05]\n","Batch:22\n","d_loss:0.17213995531346882\n","g_loss:[0.34381866, 0.34369797, 6.0343573e-05]\n","Batch:23\n","d_loss:0.16416146494339046\n","g_loss:[2.759261, 2.7591448, 5.800295e-05]\n","Batch:24\n","d_loss:0.16839866843656637\n","g_loss:[0.35738802, 0.3572762, 5.5907243e-05]\n","Batch:25\n","d_loss:0.1663214362488361\n","g_loss:[0.34422567, 0.34411776, 5.3954223e-05]\n","Batch:26\n","d_loss:0.16984815306204837\n","g_loss:[2.0275216, 2.0274186, 5.151642e-05]\n","Batch:27\n","d_loss:0.16733465259312652\n","g_loss:[0.34517676, 0.34504288, 6.6934794e-05]\n","Batch:28\n","d_loss:0.16354222975496668\n","g_loss:[0.41896784, 0.4188168, 7.551728e-05]\n","Batch:29\n","d_loss:0.1643196328659542\n","g_loss:[0.43088752, 0.43075418, 6.6665685e-05]\n","Batch:30\n","d_loss:0.16606303710432258\n","g_loss:[0.4194656, 0.41935536, 5.512506e-05]\n","Batch:31\n","d_loss:0.16295129594800528\n","g_loss:[0.39173558, 0.39163578, 4.9900205e-05]\n","Batch:32\n","d_loss:0.16373612662573578\n","g_loss:[0.35902655, 0.35893095, 4.7809004e-05]\n","Batch:33\n","d_loss:0.1653907909130794\n","g_loss:[0.9685193, 0.96842706, 4.6098467e-05]\n","Batch:34\n","d_loss:0.16623155470097117\n","g_loss:[2.637904, 2.6378143, 4.4775406e-05]\n","Batch:35\n","d_loss:0.16441617399686947\n","g_loss:[0.34668398, 0.3465595, 6.223647e-05]\n","Batch:36\n","d_loss:0.16643154431221774\n","g_loss:[0.48245794, 0.48233318, 6.2378e-05]\n","Batch:37\n","d_loss:0.16491718579709413\n","g_loss:[4.1081204, 4.1080117, 5.4246222e-05]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 365\n","Number of batches:37\n","Batch:1\n","d_loss:0.1640242152425344\n","g_loss:[3.3771582, 3.3770645, 4.6847337e-05]\n","Batch:2\n","d_loss:0.1649928536025982\n","g_loss:[2.1706219, 2.1705046, 5.861948e-05]\n","Batch:3\n","d_loss:0.16491150589718018\n","g_loss:[0.38354406, 0.38343298, 5.5539116e-05]\n","Batch:4\n","d_loss:0.5165464282035828\n","g_loss:[0.34732863, 0.34723508, 4.6774512e-05]\n","Batch:5\n","d_loss:0.16754779996699654\n","g_loss:[0.4658753, 0.46579784, 3.872659e-05]\n","Batch:6\n","d_loss:0.165437693009153\n","g_loss:[0.37567636, 0.37560844, 3.3960983e-05]\n","Batch:7\n","d_loss:0.16479866389272502\n","g_loss:[1.8200766, 1.8200136, 3.1465832e-05]\n","Batch:8\n","d_loss:0.16314384082215838\n","g_loss:[1.9423829, 1.9423218, 3.0598574e-05]\n","Batch:9\n","d_loss:0.16799453858584457\n","g_loss:[0.33589244, 0.3358283, 3.2068023e-05]\n","Batch:10\n","d_loss:0.16442680658656172\n","g_loss:[0.3339347, 0.3338703, 3.22014e-05]\n","Batch:11\n","d_loss:0.16393510217312723\n","g_loss:[0.34166864, 0.34160691, 3.08666e-05]\n","Batch:12\n","d_loss:0.1664619327057153\n","g_loss:[0.3728063, 0.3727473, 2.950684e-05]\n","Batch:13\n","d_loss:0.16361468579270877\n","g_loss:[2.822903, 2.822846, 2.845464e-05]\n","Batch:14\n","d_loss:0.16303942196100252\n","g_loss:[0.3363744, 0.33632085, 2.6771824e-05]\n","Batch:15\n","d_loss:0.16425535140297143\n","g_loss:[0.3542017, 0.3541506, 2.5551462e-05]\n","Batch:16\n","d_loss:0.20336990209762007\n","g_loss:[0.3313733, 0.33132416, 2.4566474e-05]\n","Batch:17\n","d_loss:0.1667715472940472\n","g_loss:[0.37716988, 0.37712258, 2.3644161e-05]\n","Batch:18\n","d_loss:0.16453988302964717\n","g_loss:[0.34215844, 0.34211287, 2.2786387e-05]\n","Batch:19\n","d_loss:0.16944483905172092\n","g_loss:[0.3563322, 0.3562884, 2.1898966e-05]\n","Batch:20\n","d_loss:0.1821224639134016\n","g_loss:[0.33093902, 0.33089674, 2.1148873e-05]\n","Batch:21\n","d_loss:0.1637765563937137\n","g_loss:[1.7820839, 1.782043, 2.0419371e-05]\n","Batch:22\n","d_loss:0.17125076781303505\n","g_loss:[0.35475287, 0.3547142, 1.9319736e-05]\n","Batch:23\n","d_loss:0.16431706986622885\n","g_loss:[0.46354097, 0.46350396, 1.8513208e-05]\n","Batch:24\n","d_loss:0.1668650728497596\n","g_loss:[0.43453562, 0.4344996, 1.8020963e-05]\n","Batch:25\n","d_loss:0.16506524514988996\n","g_loss:[0.37737042, 0.3773357, 1.7358389e-05]\n","Batch:26\n","d_loss:0.1686789537416189\n","g_loss:[2.477058, 2.4770243, 1.6799557e-05]\n","Batch:27\n","d_loss:0.17004347538750153\n","g_loss:[0.56983423, 0.5697997, 1.7265207e-05]\n","Batch:28\n","d_loss:0.16516642308124574\n","g_loss:[1.0951885, 1.0951512, 1.8627588e-05]\n","Batch:29\n","d_loss:0.16552053035411518\n","g_loss:[0.6739434, 0.67390597, 1.8729954e-05]\n","Batch:30\n","d_loss:0.16355667536117835\n","g_loss:[0.3404923, 0.34045416, 1.9070161e-05]\n","Batch:31\n","d_loss:0.16602237072584103\n","g_loss:[1.4397442, 1.4397063, 1.8971316e-05]\n","Batch:32\n","d_loss:0.1639593127547414\n","g_loss:[2.9760752, 2.9760354, 1.9921732e-05]\n","Batch:33\n","d_loss:0.16528801821550587\n","g_loss:[0.36870944, 0.36867142, 1.9012306e-05]\n","Batch:34\n","d_loss:0.1661994290589064\n","g_loss:[0.6652591, 0.6652231, 1.801179e-05]\n","Batch:35\n","d_loss:0.16417970083421096\n","g_loss:[0.3537925, 0.35375765, 1.7413378e-05]\n","Batch:36\n","d_loss:0.16423908039723756\n","g_loss:[0.35020524, 0.3501717, 1.6772676e-05]\n","Batch:37\n","d_loss:0.16391614720851067\n","g_loss:[0.34307718, 0.34304506, 1.606305e-05]\n","========================================\n","Epoch is: 366\n","Number of batches:37\n","Batch:1\n","d_loss:0.16416347381164087\n","g_loss:[0.37187016, 0.37183914, 1.550862e-05]\n","Batch:2\n","d_loss:0.1638832969692885\n","g_loss:[0.34726477, 0.34723455, 1.5108143e-05]\n","Batch:3\n","d_loss:0.16537629927915987\n","g_loss:[7.848241, 7.8482113, 1.472248e-05]\n","Batch:4\n","d_loss:0.5046568214893341\n","g_loss:[0.37136313, 0.37125725, 5.2945914e-05]\n","Batch:5\n","d_loss:0.16471352730877697\n","g_loss:[0.39261755, 0.39245453, 8.151511e-05]\n","Batch:6\n","d_loss:0.16501212306320667\n","g_loss:[0.5368826, 0.53672826, 7.714483e-05]\n","Batch:7\n","d_loss:0.16406417829421116\n","g_loss:[0.7094115, 0.709285, 6.323994e-05]\n","Batch:8\n","d_loss:0.16307532144128345\n","g_loss:[1.4168975, 1.4167893, 5.4149965e-05]\n","Batch:9\n","d_loss:0.16659075145798852\n","g_loss:[0.38783297, 0.38773066, 5.1149327e-05]\n","Batch:10\n","d_loss:0.16411114709626418\n","g_loss:[0.3993215, 0.399225, 4.824903e-05]\n","Batch:11\n","d_loss:0.16322639447025722\n","g_loss:[3.7218347, 3.7217503, 4.223398e-05]\n","Batch:12\n","d_loss:0.1666099429494352\n","g_loss:[0.34771633, 0.34759352, 6.140464e-05]\n","Batch:13\n","d_loss:0.16491522412979975\n","g_loss:[0.4168509, 0.41671067, 7.0116424e-05]\n","Batch:14\n","d_loss:0.16331498875661055\n","g_loss:[0.33662695, 0.3364951, 6.592082e-05]\n","Batch:15\n","d_loss:0.16367951165739214\n","g_loss:[0.9867353, 0.9866222, 5.654205e-05]\n","Batch:16\n","d_loss:0.18136818811763078\n","g_loss:[0.39652133, 0.3964173, 5.2013515e-05]\n","Batch:17\n","d_loss:0.16416000789104146\n","g_loss:[0.34734577, 0.34725103, 4.737448e-05]\n","Batch:18\n","d_loss:0.16425038292072713\n","g_loss:[1.8556921, 1.8556077, 4.220207e-05]\n","Batch:19\n","d_loss:0.16542348876828328\n","g_loss:[0.3322823, 0.33221924, 3.1531545e-05]\n","Batch:20\n","d_loss:0.17073199911101256\n","g_loss:[3.0441396, 3.0440812, 2.923823e-05]\n","Batch:21\n","d_loss:0.16941974451765418\n","g_loss:[0.35136852, 0.35131252, 2.8006118e-05]\n","Batch:22\n","d_loss:0.1717645386379445\n","g_loss:[0.6467518, 0.64669544, 2.8195018e-05]\n","Batch:23\n","d_loss:0.16491918714018539\n","g_loss:[2.8964763, 2.896421, 2.7611706e-05]\n","Batch:24\n","d_loss:0.16319063075934537\n","g_loss:[0.33368376, 0.33359993, 4.1914704e-05]\n","Batch:25\n","d_loss:0.16459287238467368\n","g_loss:[0.34602657, 0.3459447, 4.0938707e-05]\n","Batch:26\n","d_loss:0.1682611818305304\n","g_loss:[0.338577, 0.3385159, 3.0554664e-05]\n","Batch:27\n","d_loss:0.16438121731334832\n","g_loss:[0.5666254, 0.56657296, 2.6224909e-05]\n","Batch:28\n","d_loss:0.16389884162708768\n","g_loss:[0.34292546, 0.34285024, 3.761131e-05]\n","Batch:29\n","d_loss:0.16551421301119262\n","g_loss:[0.64988816, 0.64978904, 4.95676e-05]\n","Batch:30\n","d_loss:0.16627301914559212\n","g_loss:[0.3513828, 0.351277, 5.289382e-05]\n","Batch:31\n","d_loss:0.16291205004381482\n","g_loss:[0.71737725, 0.7172771, 5.005769e-05]\n","Batch:32\n","d_loss:0.16313972634088714\n","g_loss:[0.33765352, 0.33756196, 4.577506e-05]\n","Batch:33\n","d_loss:0.16307806889562926\n","g_loss:[0.33865926, 0.33857858, 4.0333307e-05]\n","Batch:34\n","d_loss:0.16299517223069415\n","g_loss:[2.1462333, 2.1461635, 3.4888417e-05]\n","Batch:35\n","d_loss:0.16317667542898562\n","g_loss:[0.8303519, 0.8302885, 3.1682575e-05]\n","Batch:36\n","d_loss:0.16341240170913807\n","g_loss:[1.3294708, 1.3294016, 3.4599394e-05]\n","Batch:37\n","d_loss:0.1638156895114662\n","g_loss:[0.3312956, 0.33120733, 4.4141783e-05]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 367\n","Number of batches:37\n","Batch:1\n","d_loss:0.16293889271037187\n","g_loss:[0.35577333, 0.35567954, 4.689407e-05]\n","Batch:2\n","d_loss:0.16407459370930155\n","g_loss:[2.2184446, 2.2183573, 4.3643588e-05]\n","Batch:3\n","d_loss:0.16362731642038852\n","g_loss:[0.3514554, 0.35134268, 5.6361354e-05]\n","Batch:4\n","d_loss:0.5475154966115952\n","g_loss:[2.269145, 2.2690363, 5.43021e-05]\n","Batch:5\n","d_loss:0.1637003914656816\n","g_loss:[0.3585026, 0.3584215, 4.0542804e-05]\n","Batch:6\n","d_loss:0.1645809012989048\n","g_loss:[0.35812795, 0.35805506, 3.644167e-05]\n","Batch:7\n","d_loss:0.16297876298631309\n","g_loss:[0.36689943, 0.36683214, 3.365139e-05]\n","Batch:8\n","d_loss:0.16326816059154226\n","g_loss:[0.8715017, 0.8714384, 3.16563e-05]\n","Batch:9\n","d_loss:0.16383260803922894\n","g_loss:[0.45270967, 0.4526358, 3.6943275e-05]\n","Batch:10\n","d_loss:0.16604398150229827\n","g_loss:[0.44538823, 0.445315, 3.6617465e-05]\n","Batch:11\n","d_loss:0.16459004523494514\n","g_loss:[0.56121105, 0.5611454, 3.2824475e-05]\n","Batch:12\n","d_loss:0.16450532737508183\n","g_loss:[0.34395126, 0.3438852, 3.3018136e-05]\n","Batch:13\n","d_loss:0.1633515106223058\n","g_loss:[1.7074009, 1.7073386, 3.1199044e-05]\n","Batch:14\n","d_loss:0.16290159615164157\n","g_loss:[2.9212413, 2.9211607, 4.0240942e-05]\n","Batch:15\n","d_loss:0.1642494426560006\n","g_loss:[0.3699451, 0.36984265, 5.1234783e-05]\n","Batch:16\n","d_loss:0.17609618848655373\n","g_loss:[0.34510615, 0.34500176, 5.2197476e-05]\n","Batch:17\n","d_loss:0.16277461539721116\n","g_loss:[2.0938423, 2.093748, 4.7040183e-05]\n","Batch:18\n","d_loss:0.16716220416128635\n","g_loss:[0.34943858, 0.34932622, 5.617689e-05]\n","Batch:19\n","d_loss:0.16448755566307227\n","g_loss:[0.3485995, 0.3484906, 5.4453023e-05]\n","Batch:20\n","d_loss:0.16824476928741205\n","g_loss:[1.0799987, 1.079906, 4.6348163e-05]\n","Batch:21\n","d_loss:0.1689564148109639\n","g_loss:[0.34362954, 0.34349948, 6.502323e-05]\n","Batch:22\n","d_loss:0.169445391438785\n","g_loss:[0.33580098, 0.3356643, 6.834065e-05]\n","Batch:23\n","d_loss:0.16362123560975306\n","g_loss:[0.34828547, 0.34816223, 6.1619445e-05]\n","Batch:24\n","d_loss:0.16333608301829372\n","g_loss:[0.33442646, 0.3343217, 5.2372787e-05]\n","Batch:25\n","d_loss:0.16389311147941044\n","g_loss:[0.36986703, 0.36978024, 4.3387114e-05]\n","Batch:26\n","d_loss:0.16704268290050095\n","g_loss:[0.33406994, 0.3339942, 3.7861053e-05]\n","Batch:27\n","d_loss:0.16454020771197975\n","g_loss:[0.34129754, 0.34123057, 3.34768e-05]\n","Batch:28\n","d_loss:0.1641800754077849\n","g_loss:[0.5694936, 0.5694343, 2.9667219e-05]\n","Batch:29\n","d_loss:0.16291077474306803\n","g_loss:[2.1486905, 2.1486337, 2.842764e-05]\n","Batch:30\n","d_loss:0.16523701634287136\n","g_loss:[0.3369993, 0.3369516, 2.3843077e-05]\n","Batch:31\n","d_loss:0.1628730218799319\n","g_loss:[0.33449876, 0.3344544, 2.2188196e-05]\n","Batch:32\n","d_loss:0.16300730532384478\n","g_loss:[1.7906612, 1.7906189, 2.117863e-05]\n","Batch:33\n","d_loss:0.1631267357370234\n","g_loss:[0.34197357, 0.34192267, 2.5452384e-05]\n","Batch:34\n","d_loss:0.1633116717584926\n","g_loss:[0.34813324, 0.34807622, 2.8508086e-05]\n","Batch:35\n","d_loss:0.1631361112158629\n","g_loss:[0.49427685, 0.49421903, 2.8914648e-05]\n","Batch:36\n","d_loss:0.16385974642071233\n","g_loss:[6.8532476, 6.853191, 2.8253047e-05]\n","Batch:37\n","d_loss:0.16366562254370365\n","g_loss:[3.067416, 3.0671723, 0.00012184473]\n","========================================\n","Epoch is: 368\n","Number of batches:37\n","Batch:1\n","d_loss:0.16278184427210363\n","g_loss:[6.969145, 6.9687347, 0.0002050572]\n","Batch:2\n","d_loss:0.163948386823904\n","g_loss:[0.38671464, 0.38606602, 0.00032430352]\n","Batch:3\n","d_loss:0.16370028059463948\n","g_loss:[0.378607, 0.37787986, 0.00036357658]\n","Batch:4\n","d_loss:0.5499468296766281\n","g_loss:[0.39548954, 0.39486793, 0.00031080417]\n","Batch:5\n","d_loss:0.16472495401103515\n","g_loss:[0.42990714, 0.42943513, 0.00023600929]\n","Batch:6\n","d_loss:0.1643479453632608\n","g_loss:[3.7468345, 3.7464614, 0.00018656989]\n","Batch:7\n","d_loss:0.1629740436837892\n","g_loss:[3.6426709, 3.642346, 0.00016249297]\n","Batch:8\n","d_loss:0.17800153605639935\n","g_loss:[0.49270523, 0.49240178, 0.0001517279]\n","Batch:9\n","d_loss:0.1659099554817658\n","g_loss:[3.0763328, 3.0760431, 0.00014485848]\n","Batch:10\n","d_loss:0.1741307338452316\n","g_loss:[0.49138165, 0.491122, 0.0001298252]\n","Batch:11\n","d_loss:0.16512665734626353\n","g_loss:[0.33980426, 0.33952582, 0.00013922379]\n","Batch:12\n","d_loss:0.1700283566460712\n","g_loss:[0.33938277, 0.33909458, 0.00014408866]\n","Batch:13\n","d_loss:0.17966740299016237\n","g_loss:[0.3465569, 0.34627408, 0.00014140771]\n","Batch:14\n","d_loss:0.17446415033191442\n","g_loss:[0.3644083, 0.36413696, 0.00013567132]\n","Batch:15\n","d_loss:0.16433386743301526\n","g_loss:[0.3806685, 0.3804081, 0.00013018896]\n","Batch:16\n","d_loss:0.16734143369831145\n","g_loss:[1.0580459, 1.0577937, 0.00012603783]\n","Batch:17\n","d_loss:0.16772188809409272\n","g_loss:[0.37016156, 0.3699149, 0.00012333423]\n","Batch:18\n","d_loss:0.16874647134682164\n","g_loss:[3.454179, 3.4539375, 0.00012076675]\n","Batch:19\n","d_loss:0.17413422113168053\n","g_loss:[0.3432677, 0.34302926, 0.0001192195]\n","Batch:20\n","d_loss:0.1784427878446877\n","g_loss:[0.395889, 0.39565736, 0.000115824114]\n","Batch:21\n","d_loss:0.16450506395631237\n","g_loss:[0.38927004, 0.38904724, 0.000111400695]\n","Batch:22\n","d_loss:0.16715249291883083\n","g_loss:[0.34973237, 0.34951696, 0.0001077118]\n","Batch:23\n","d_loss:0.16506402258164599\n","g_loss:[0.3799808, 0.3797717, 0.00010454614]\n","Batch:24\n","d_loss:0.16743263116222806\n","g_loss:[0.36170408, 0.36150068, 0.000101695514]\n","Batch:25\n","d_loss:0.16507741826353595\n","g_loss:[0.35654542, 0.35634756, 9.8934455e-05]\n","Batch:26\n","d_loss:0.16627814853200107\n","g_loss:[0.35337913, 0.35318643, 9.635036e-05]\n","Batch:27\n","d_loss:0.17016903990588617\n","g_loss:[0.3709186, 0.37073094, 9.382674e-05]\n","Batch:28\n","d_loss:0.16442014212952927\n","g_loss:[0.34152088, 0.34133813, 9.136804e-05]\n","Batch:29\n","d_loss:0.16724979302671272\n","g_loss:[0.35590947, 0.35573155, 8.8958375e-05]\n","Batch:30\n","d_loss:0.16392157302470878\n","g_loss:[0.35487738, 0.35470408, 8.664837e-05]\n","Batch:31\n","d_loss:0.17438960438448703\n","g_loss:[0.38502496, 0.3848562, 8.438943e-05]\n","Batch:32\n","d_loss:0.16311841361311963\n","g_loss:[0.67062294, 0.67045856, 8.219768e-05]\n","Batch:33\n","d_loss:0.16520439842861379\n","g_loss:[0.5287563, 0.5285952, 8.055585e-05]\n","Batch:34\n","d_loss:0.17112977495708037\n","g_loss:[0.34585357, 0.34569526, 7.915586e-05]\n","Batch:35\n","d_loss:0.1746927799686091\n","g_loss:[0.35915512, 0.35899973, 7.76945e-05]\n","Batch:36\n","d_loss:0.16422421194147319\n","g_loss:[0.36822262, 0.368071, 7.582428e-05]\n","Batch:37\n","d_loss:0.16566369821157423\n","g_loss:[1.5576726, 1.5575253, 7.369171e-05]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 369\n","Number of batches:37\n","Batch:1\n","d_loss:0.16389095356862526\n","g_loss:[0.34783113, 0.34768552, 7.280565e-05]\n","Batch:2\n","d_loss:0.17249898951558862\n","g_loss:[0.36386508, 0.36372173, 7.168113e-05]\n","Batch:3\n","d_loss:0.1701392270042561\n","g_loss:[0.33224535, 0.3321062, 6.957503e-05]\n","Batch:4\n","d_loss:0.5333874672651291\n","g_loss:[0.37730595, 0.3771718, 6.7063185e-05]\n","Batch:5\n","d_loss:0.17179387372743804\n","g_loss:[0.36343688, 0.36330718, 6.484601e-05]\n","Batch:6\n","d_loss:0.16451705567305908\n","g_loss:[0.37767512, 0.3775493, 6.2909654e-05]\n","Batch:7\n","d_loss:0.16390151978703216\n","g_loss:[0.39325666, 0.39313412, 6.126999e-05]\n","Batch:8\n","d_loss:0.163273410347756\n","g_loss:[0.39449096, 0.39437172, 5.96193e-05]\n","Batch:9\n","d_loss:0.1669355758040183\n","g_loss:[0.36997885, 0.36986268, 5.80855e-05]\n","Batch:10\n","d_loss:0.16867018326593097\n","g_loss:[3.170896, 3.1707828, 5.663725e-05]\n","Batch:11\n","d_loss:0.16443422053271206\n","g_loss:[0.33536664, 0.33525568, 5.5479042e-05]\n","Batch:12\n","d_loss:0.1662110098041012\n","g_loss:[1.5759608, 1.5758523, 5.4232838e-05]\n","Batch:13\n","d_loss:0.17012871126644313\n","g_loss:[2.148028, 2.1479182, 5.4841432e-05]\n","Batch:14\n","d_loss:0.16365371586289257\n","g_loss:[0.35688114, 0.35675895, 6.109758e-05]\n","Batch:15\n","d_loss:0.16497557820548536\n","g_loss:[0.36621487, 0.36609313, 6.087163e-05]\n","Batch:16\n","d_loss:0.17523907753638923\n","g_loss:[0.43023962, 0.4301272, 5.6203102e-05]\n","Batch:17\n","d_loss:0.17179364527692087\n","g_loss:[0.36777675, 0.36767334, 5.170783e-05]\n","Batch:18\n","d_loss:0.16912284831050783\n","g_loss:[8.863032, 8.862935, 4.8589805e-05]\n","Batch:19\n","d_loss:0.17223366891266778\n","g_loss:[0.4169358, 0.41683778, 4.901106e-05]\n","Batch:20\n","d_loss:0.17003903748991434\n","g_loss:[3.3821666, 3.3820653, 5.06941e-05]\n","Batch:21\n","d_loss:0.1663103308674181\n","g_loss:[0.33702782, 0.33692634, 5.0738592e-05]\n","Batch:22\n","d_loss:0.17042763251811266\n","g_loss:[0.33560196, 0.33550075, 5.060118e-05]\n","Batch:23\n","d_loss:0.16983850964606972\n","g_loss:[0.34265646, 0.34255713, 4.9672723e-05]\n","Batch:24\n","d_loss:0.16332361259264871\n","g_loss:[9.900847, 9.900751, 4.8303307e-05]\n","Batch:25\n","d_loss:0.16344914506044006\n","g_loss:[0.42090943, 0.4207589, 7.526872e-05]\n","Batch:26\n","d_loss:0.16424048030876293\n","g_loss:[2.1170423, 2.1168523, 9.503765e-05]\n","Batch:27\n","d_loss:0.1637907298609207\n","g_loss:[0.35615307, 0.35598394, 8.455711e-05]\n","Batch:28\n","d_loss:0.16407342985621653\n","g_loss:[0.35871083, 0.35856837, 7.122135e-05]\n","Batch:29\n","d_loss:0.1634311518355389\n","g_loss:[0.36818436, 0.36806488, 5.9742884e-05]\n","Batch:30\n","d_loss:0.16276246398047078\n","g_loss:[0.33892182, 0.3388185, 5.1668925e-05]\n","Batch:31\n","d_loss:0.16315929104894167\n","g_loss:[0.3320851, 0.33199227, 4.641666e-05]\n","Batch:32\n","d_loss:0.16336102537025\n","g_loss:[0.33752066, 0.33743432, 4.3170454e-05]\n","Batch:33\n","d_loss:0.1635575844120467\n","g_loss:[0.36538145, 0.3652992, 4.1128093e-05]\n","Batch:34\n","d_loss:0.16356342550443514\n","g_loss:[0.34589666, 0.34581733, 3.9672963e-05]\n","Batch:35\n","d_loss:0.16705931191972923\n","g_loss:[0.33842266, 0.33834562, 3.8518858e-05]\n","Batch:36\n","d_loss:0.1668795510122436\n","g_loss:[0.33988044, 0.33980545, 3.7497924e-05]\n","Batch:37\n","d_loss:0.16468415529379854\n","g_loss:[1.1540706, 1.1539975, 3.6553207e-05]\n","========================================\n","Epoch is: 370\n","Number of batches:37\n","Batch:1\n","d_loss:0.16397348810642143\n","g_loss:[0.6443877, 0.64431727, 3.523588e-05]\n","Batch:2\n","d_loss:0.1630483562257723\n","g_loss:[0.33640218, 0.33633327, 3.4449004e-05]\n","Batch:3\n","d_loss:0.17274411849211901\n","g_loss:[0.34832293, 0.34825557, 3.3675293e-05]\n","Batch:4\n","d_loss:0.5525055527687073\n","g_loss:[1.5209525, 1.5208868, 3.284603e-05]\n","Batch:5\n","d_loss:0.1760731537360698\n","g_loss:[0.33802477, 0.33795887, 3.2951913e-05]\n","Batch:6\n","d_loss:0.16522153257392347\n","g_loss:[0.39264673, 0.39258116, 3.277638e-05]\n","Batch:7\n","d_loss:0.16417918029765133\n","g_loss:[0.3764021, 0.37633783, 3.2148302e-05]\n","Batch:8\n","d_loss:0.1631586647563381\n","g_loss:[0.41607893, 0.41601652, 3.1198575e-05]\n","Batch:9\n","d_loss:0.17722030107324827\n","g_loss:[0.36094785, 0.36088738, 3.0229347e-05]\n","Batch:10\n","d_loss:0.1666313044333947\n","g_loss:[0.37276354, 0.3727048, 2.9369741e-05]\n","Batch:11\n","d_loss:0.16589206673961598\n","g_loss:[0.33826527, 0.33820695, 2.9166204e-05]\n","Batch:12\n","d_loss:0.16912106747622602\n","g_loss:[0.35578546, 0.35572842, 2.8519007e-05]\n","Batch:13\n","d_loss:0.16404967024573125\n","g_loss:[1.9452132, 1.9451576, 2.7776296e-05]\n","Batch:14\n","d_loss:0.1632673282183532\n","g_loss:[0.3358363, 0.3357269, 5.4707518e-05]\n","Batch:15\n","d_loss:0.1655328740052937\n","g_loss:[0.779685, 0.77954936, 6.783321e-05]\n","Batch:16\n","d_loss:0.17366266809403896\n","g_loss:[0.3393528, 0.33919492, 7.8926474e-05]\n","Batch:17\n","d_loss:0.16347365358888055\n","g_loss:[0.44778433, 0.447654, 6.516755e-05]\n","Batch:18\n","d_loss:0.16349758123396896\n","g_loss:[0.34261647, 0.34251434, 5.1073148e-05]\n","Batch:19\n","d_loss:0.16322107904125005\n","g_loss:[0.36011237, 0.36003482, 3.8779726e-05]\n","Batch:20\n","d_loss:0.16393276423332281\n","g_loss:[0.40455106, 0.4044869, 3.2077794e-05]\n","Batch:21\n","d_loss:0.16559010468336055\n","g_loss:[2.382477, 2.3824196, 2.8686307e-05]\n","Batch:22\n","d_loss:0.1694160378538072\n","g_loss:[0.35237953, 0.35231632, 3.1606276e-05]\n","Batch:23\n","d_loss:0.16551284572051372\n","g_loss:[0.34249115, 0.34242466, 3.3248663e-05]\n","Batch:24\n","d_loss:0.16562443835573504\n","g_loss:[0.3667896, 0.36672533, 3.2138356e-05]\n","Batch:25\n","d_loss:0.16438672080766992\n","g_loss:[0.3589391, 0.35887882, 3.0138344e-05]\n","Batch:26\n","d_loss:0.16560910490807146\n","g_loss:[0.3415485, 0.34149218, 2.8168797e-05]\n","Batch:27\n","d_loss:0.16287012042630522\n","g_loss:[0.34939653, 0.3493436, 2.6458714e-05]\n","Batch:28\n","d_loss:0.16360466211335734\n","g_loss:[0.38517967, 0.3851292, 2.5230256e-05]\n","Batch:29\n","d_loss:0.16327669748170592\n","g_loss:[0.36443162, 0.3643832, 2.4212331e-05]\n","Batch:30\n","d_loss:0.1632187276336481\n","g_loss:[0.35042587, 0.35037893, 2.3474327e-05]\n","Batch:31\n","d_loss:0.16501579273608513\n","g_loss:[0.37118512, 0.3711394, 2.2865297e-05]\n","Batch:32\n","d_loss:0.16385624633039697\n","g_loss:[0.80816084, 0.8081162, 2.2326369e-05]\n","Batch:33\n","d_loss:0.1640828299914574\n","g_loss:[0.34873405, 0.3486879, 2.308207e-05]\n","Batch:34\n","d_loss:0.17084362658351893\n","g_loss:[0.3595505, 0.3595047, 2.2902426e-05]\n","Batch:35\n","d_loss:0.1659261976601556\n","g_loss:[0.34303218, 0.34298882, 2.1687205e-05]\n","Batch:36\n","d_loss:0.1670794814999681\n","g_loss:[0.38117012, 0.38112855, 2.079307e-05]\n","Batch:37\n","d_loss:0.16337614530857536\n","g_loss:[0.36452153, 0.36448085, 2.0334695e-05]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 371\n","Number of batches:37\n","Batch:1\n","d_loss:0.1642035718105035\n","g_loss:[0.35935456, 0.35931468, 1.993196e-05]\n","Batch:2\n","d_loss:0.1641832474124385\n","g_loss:[0.34525982, 0.3452208, 1.9505927e-05]\n","Batch:3\n","d_loss:0.1637133390904637\n","g_loss:[0.34950155, 0.34946334, 1.9097854e-05]\n","Batch:4\n","d_loss:0.617321103811264\n","g_loss:[3.2809885, 3.280951, 1.8693641e-05]\n","Batch:5\n","d_loss:0.17414153384743258\n","g_loss:[0.33777326, 0.33773664, 1.831605e-05]\n","Batch:6\n","d_loss:0.1647338735347148\n","g_loss:[0.36440387, 0.36436796, 1.795156e-05]\n","Batch:7\n","d_loss:0.16419554337335285\n","g_loss:[0.34138426, 0.34134907, 1.7599034e-05]\n","Batch:8\n","d_loss:0.16460960515541956\n","g_loss:[0.3666028, 0.36656836, 1.7230503e-05]\n","Batch:9\n","d_loss:0.16341242306225467\n","g_loss:[0.35521328, 0.35517952, 1.6885748e-05]\n","Batch:10\n","d_loss:0.1683953869069228\n","g_loss:[0.3427331, 0.34269997, 1.6558382e-05]\n","Batch:11\n","d_loss:0.16749731134041212\n","g_loss:[0.36724368, 0.36721125, 1.620535e-05]\n","Batch:12\n","d_loss:0.16318871587645845\n","g_loss:[0.33516335, 0.33513165, 1.585975e-05]\n","Batch:13\n","d_loss:0.1660389245953411\n","g_loss:[0.35611492, 0.35608387, 1.553002e-05]\n","Batch:14\n","d_loss:0.16326428596221376\n","g_loss:[0.77558696, 0.77555645, 1.5246391e-05]\n","Batch:15\n","d_loss:0.16347034733917098\n","g_loss:[0.33705968, 0.33701748, 2.1101168e-05]\n","Batch:16\n","d_loss:0.17900642496533692\n","g_loss:[0.3573525, 0.35729945, 2.6529575e-05]\n","Batch:17\n","d_loss:0.1692161201426643\n","g_loss:[0.35626453, 0.35621035, 2.7096e-05]\n","Batch:18\n","d_loss:0.1636582802166231\n","g_loss:[0.36558184, 0.3655327, 2.4569628e-05]\n","Batch:19\n","d_loss:0.16345086670480669\n","g_loss:[0.34480816, 0.34476537, 2.1394277e-05]\n","Batch:20\n","d_loss:0.1675393381228787\n","g_loss:[0.35341448, 0.35337687, 1.8807314e-05]\n","Batch:21\n","d_loss:0.16355347042554058\n","g_loss:[0.35267758, 0.35264355, 1.702414e-05]\n","Batch:22\n","d_loss:0.16395231497881468\n","g_loss:[0.3376853, 0.33765358, 1.5853959e-05]\n","Batch:23\n","d_loss:0.1673064728920508\n","g_loss:[0.3408069, 0.34077665, 1.5131284e-05]\n","Batch:24\n","d_loss:0.16555157052061986\n","g_loss:[0.35842365, 0.3583945, 1.4569006e-05]\n","Batch:25\n","d_loss:0.16613373649306595\n","g_loss:[0.35257226, 0.35254356, 1.4342972e-05]\n","Batch:26\n","d_loss:0.16419921872238774\n","g_loss:[0.3484085, 0.34838057, 1.3966514e-05]\n","Batch:27\n","d_loss:0.1633630724318209\n","g_loss:[0.6754144, 0.6753865, 1.3957919e-05]\n","Batch:28\n","d_loss:0.1634726938282256\n","g_loss:[0.342982, 0.34289205, 4.4972683e-05]\n","Batch:29\n","d_loss:0.16341297188682802\n","g_loss:[2.660301, 2.6601355, 8.27106e-05]\n","Batch:30\n","d_loss:0.16319873621068837\n","g_loss:[0.34870216, 0.3483759, 0.00016313336]\n","Batch:31\n","d_loss:0.16361083755691652\n","g_loss:[0.3866911, 0.38626117, 0.00021496379]\n","Batch:32\n","d_loss:0.16310168220297783\n","g_loss:[0.33413425, 0.33373028, 0.00020198809]\n","Batch:33\n","d_loss:0.16332602373768168\n","g_loss:[0.34290883, 0.3425853, 0.00016176632]\n","Batch:34\n","d_loss:0.16447590308234794\n","g_loss:[0.33674285, 0.33649302, 0.00012491831]\n","Batch:35\n","d_loss:0.16299271973912255\n","g_loss:[0.36666882, 0.36647084, 9.899406e-05]\n","Batch:36\n","d_loss:0.16333461268823157\n","g_loss:[3.5471203, 3.546959, 8.068419e-05]\n","Batch:37\n","d_loss:0.16363108134828508\n","g_loss:[0.33612567, 0.33597416, 7.575939e-05]\n","========================================\n","Epoch is: 372\n","Number of batches:37\n","Batch:1\n","d_loss:0.1637819547959225\n","g_loss:[0.34046724, 0.3403214, 7.292902e-05]\n","Batch:2\n","d_loss:0.16327739196094626\n","g_loss:[0.3546108, 0.3544805, 6.51515e-05]\n","Batch:3\n","d_loss:0.1633395715361985\n","g_loss:[0.34066662, 0.34055287, 5.6872144e-05]\n","Batch:4\n","d_loss:0.5785669833421707\n","g_loss:[0.3594464, 0.35934696, 4.9723156e-05]\n","Batch:5\n","d_loss:0.1697295999911148\n","g_loss:[1.7036763, 1.703588, 4.418176e-05]\n","Batch:6\n","d_loss:0.16730876579822507\n","g_loss:[0.36832172, 0.36820978, 5.596644e-05]\n","Batch:7\n","d_loss:0.16563439443416428\n","g_loss:[0.34936094, 0.3492272, 6.6879475e-05]\n","Batch:8\n","d_loss:0.16621697261871304\n","g_loss:[0.33318904, 0.33306628, 6.1380684e-05]\n","Batch:9\n","d_loss:0.16457870590784296\n","g_loss:[0.36676243, 0.36665922, 5.160887e-05]\n","Batch:10\n","d_loss:0.16497916576918215\n","g_loss:[0.3490807, 0.3489904, 4.5152432e-05]\n","Batch:11\n","d_loss:0.16656535895890556\n","g_loss:[0.33033177, 0.33024982, 4.097188e-05]\n","Batch:12\n","d_loss:0.16502350746304728\n","g_loss:[0.33068138, 0.33060482, 3.8287046e-05]\n","Batch:13\n","d_loss:0.16782586424960755\n","g_loss:[0.33834982, 0.338277, 3.6398113e-05]\n","Batch:14\n","d_loss:0.1630320251279045\n","g_loss:[0.35109985, 0.35103, 3.4929733e-05]\n","Batch:15\n","d_loss:0.16624075591971632\n","g_loss:[0.32617003, 0.32610267, 3.3669286e-05]\n","Batch:16\n","d_loss:0.171390101313591\n","g_loss:[0.3393074, 0.33924234, 3.2526026e-05]\n","Batch:17\n","d_loss:0.1647327861210215\n","g_loss:[0.37072593, 0.37066302, 3.1463e-05]\n","Batch:18\n","d_loss:0.16359650713275187\n","g_loss:[0.33648327, 0.33642218, 3.054481e-05]\n","Batch:19\n","d_loss:0.163510190182933\n","g_loss:[0.3433687, 0.34330943, 2.9636903e-05]\n","Batch:20\n","d_loss:0.16337983650737442\n","g_loss:[0.3279069, 0.32784933, 2.8793766e-05]\n","Batch:21\n","d_loss:0.16353018968948163\n","g_loss:[0.3323493, 0.33229333, 2.7980852e-05]\n","Batch:22\n","d_loss:0.16443292559779366\n","g_loss:[0.35378274, 0.35372835, 2.7197942e-05]\n","Batch:23\n","d_loss:0.16557322032531374\n","g_loss:[0.33659664, 0.33654374, 2.6448548e-05]\n","Batch:24\n","d_loss:0.1709484474704368\n","g_loss:[0.32634306, 0.32629156, 2.5754474e-05]\n","Batch:25\n","d_loss:0.16352231006021611\n","g_loss:[0.32933524, 0.32928512, 2.507118e-05]\n","Batch:26\n","d_loss:0.1634317279631432\n","g_loss:[3.4733422, 3.4732933, 2.4430485e-05]\n","Batch:27\n","d_loss:0.1651338349802245\n","g_loss:[0.341506, 0.3414537, 2.6150108e-05]\n","Batch:28\n","d_loss:0.16535422098240815\n","g_loss:[0.59225845, 0.5922024, 2.801654e-05]\n","Batch:29\n","d_loss:0.16497395956685068\n","g_loss:[0.33508867, 0.335033, 2.7834512e-05]\n","Batch:30\n","d_loss:0.1629882654924586\n","g_loss:[0.9318103, 0.9317557, 2.7307e-05]\n","Batch:31\n","d_loss:0.16331245983747067\n","g_loss:[0.33029056, 0.33014613, 7.221506e-05]\n","Batch:32\n","d_loss:0.1633233428292442\n","g_loss:[0.3328948, 0.33264285, 0.00012597404]\n","Batch:33\n","d_loss:0.1633517158261384\n","g_loss:[0.33196235, 0.33167213, 0.00014510837]\n","Batch:34\n","d_loss:0.16281566754696541\n","g_loss:[0.33380595, 0.33352876, 0.0001385992]\n","Batch:35\n","d_loss:0.16397887247876497\n","g_loss:[0.39507172, 0.39482617, 0.00012277428]\n","Batch:36\n","d_loss:0.16298516008282604\n","g_loss:[0.33690718, 0.33669358, 0.000106798536]\n","Batch:37\n","d_loss:0.16447716404582025\n","g_loss:[2.869953, 2.8697639, 9.448547e-05]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 373\n","Number of batches:37\n","Batch:1\n","d_loss:0.16408441487146774\n","g_loss:[0.3511138, 0.35091013, 0.00010183331]\n","Batch:2\n","d_loss:0.16485256554551597\n","g_loss:[0.33073002, 0.33052087, 0.00010457945]\n","Batch:3\n","d_loss:0.16353590508151683\n","g_loss:[0.34015757, 0.33995593, 0.00010082046]\n","Batch:4\n","d_loss:0.5819201320409775\n","g_loss:[0.36061454, 0.36042598, 9.42755e-05]\n","Batch:5\n","d_loss:0.17231954681847128\n","g_loss:[0.36088148, 0.36070707, 8.720229e-05]\n","Batch:6\n","d_loss:0.16556629218393937\n","g_loss:[1.0823979, 1.0822351, 8.1432285e-05]\n","Batch:7\n","d_loss:0.16310452322795754\n","g_loss:[1.4031794, 1.4029359, 0.000121760735]\n","Batch:8\n","d_loss:0.16455151018453762\n","g_loss:[0.3327025, 0.33215502, 0.00027373695]\n","Batch:9\n","d_loss:0.16405430692975642\n","g_loss:[0.34720603, 0.3465755, 0.00031526174]\n","Batch:10\n","d_loss:0.1637687288748566\n","g_loss:[0.36032102, 0.35981983, 0.00025059836]\n","Batch:11\n","d_loss:0.16575239073790726\n","g_loss:[0.33664775, 0.33629757, 0.00017509202]\n","Batch:12\n","d_loss:0.1690066999217379\n","g_loss:[0.32983553, 0.32958096, 0.00012727965]\n","Batch:13\n","d_loss:0.16297648998443037\n","g_loss:[0.33154204, 0.33133343, 0.000104303625]\n","Batch:14\n","d_loss:0.16305954713971005\n","g_loss:[4.04319, 4.0430017, 9.424231e-05]\n","Batch:15\n","d_loss:0.16422747345131938\n","g_loss:[0.3433162, 0.34314102, 8.758597e-05]\n","Batch:16\n","d_loss:0.17349764864775352\n","g_loss:[0.342686, 0.34251606, 8.49722e-05]\n","Batch:17\n","d_loss:0.16425520286429673\n","g_loss:[0.35448724, 0.35432115, 8.304159e-05]\n","Batch:18\n","d_loss:0.164008769497741\n","g_loss:[0.3548437, 0.35468107, 8.131492e-05]\n","Batch:19\n","d_loss:0.16315902595670195\n","g_loss:[0.34280938, 0.34265012, 7.9624595e-05]\n","Batch:20\n","d_loss:0.1706761191599071\n","g_loss:[0.34498957, 0.34483364, 7.7959245e-05]\n","Batch:21\n","d_loss:0.163693683774909\n","g_loss:[0.35564315, 0.35549045, 7.6354845e-05]\n","Batch:22\n","d_loss:0.16565512504894286\n","g_loss:[0.3486577, 0.3485083, 7.4699754e-05]\n","Batch:23\n","d_loss:0.16887038827553624\n","g_loss:[0.341724, 0.3415777, 7.315182e-05]\n","Batch:24\n","d_loss:0.16471411638485733\n","g_loss:[0.3504586, 0.35031527, 7.1654555e-05]\n","Batch:25\n","d_loss:0.16428108420222998\n","g_loss:[1.1370696, 1.1369292, 7.020075e-05]\n","Batch:26\n","d_loss:0.16556243392369652\n","g_loss:[0.43453947, 0.43440256, 6.8455534e-05]\n","Batch:27\n","d_loss:0.164751459400577\n","g_loss:[0.89178413, 0.8916508, 6.667504e-05]\n","Batch:28\n","d_loss:0.16729544398549479\n","g_loss:[0.386429, 0.38629547, 6.6772285e-05]\n","Batch:29\n","d_loss:0.16329021575802471\n","g_loss:[0.33694997, 0.33681434, 6.781417e-05]\n","Batch:30\n","d_loss:0.16300223320286022\n","g_loss:[0.35827607, 0.35813993, 6.806277e-05]\n","Batch:31\n","d_loss:0.1652091642281448\n","g_loss:[0.3376271, 0.33749282, 6.7147936e-05]\n","Batch:32\n","d_loss:0.1634283239582146\n","g_loss:[0.34123755, 0.3411063, 6.563208e-05]\n","Batch:33\n","d_loss:0.163702819532773\n","g_loss:[0.33972117, 0.33959296, 6.4098494e-05]\n","Batch:34\n","d_loss:0.16291121327958535\n","g_loss:[0.33122092, 0.3310956, 6.265819e-05]\n","Batch:35\n","d_loss:0.16348520651081344\n","g_loss:[0.3465135, 0.3463908, 6.134967e-05]\n","Batch:36\n","d_loss:0.16341134960384807\n","g_loss:[0.47444776, 0.4743276, 6.0088147e-05]\n","Batch:37\n","d_loss:0.16378459382394794\n","g_loss:[0.33764097, 0.33751717, 6.189702e-05]\n","========================================\n","Epoch is: 374\n","Number of batches:37\n","Batch:1\n","d_loss:0.1629614386256435\n","g_loss:[1.4118859, 1.4117632, 6.13586e-05]\n","Batch:2\n","d_loss:0.16485065878441674\n","g_loss:[0.4195085, 0.41939616, 5.6164732e-05]\n","Batch:3\n","d_loss:0.1629901378764771\n","g_loss:[0.34797153, 0.34786433, 5.3596836e-05]\n","Batch:4\n","d_loss:0.5709352344274521\n","g_loss:[0.39228946, 0.39218462, 5.242211e-05]\n","Batch:5\n","d_loss:0.16893902317679022\n","g_loss:[0.33908582, 0.33898288, 5.1473184e-05]\n","Batch:6\n","d_loss:0.16761063929880038\n","g_loss:[0.36867315, 0.36857194, 5.0601742e-05]\n","Batch:7\n","d_loss:0.1656916399806505\n","g_loss:[0.46146798, 0.46136862, 4.9682058e-05]\n","Batch:8\n","d_loss:0.16358824123744853\n","g_loss:[2.3237119, 2.3236141, 4.88202e-05]\n","Batch:9\n","d_loss:0.16324251385231037\n","g_loss:[0.45753473, 0.45743638, 4.9176335e-05]\n","Batch:10\n","d_loss:0.16365592890360858\n","g_loss:[0.7113927, 0.71129286, 4.9923212e-05]\n","Batch:11\n","d_loss:0.1649223007625551\n","g_loss:[0.3332914, 0.33319303, 4.9181963e-05]\n","Batch:12\n","d_loss:0.16472825777964317\n","g_loss:[0.34413865, 0.34404182, 4.8407506e-05]\n","Batch:13\n","d_loss:0.16334316474967636\n","g_loss:[1.0262234, 1.0261292, 4.7068293e-05]\n","Batch:14\n","d_loss:0.1632143708266085\n","g_loss:[0.35477248, 0.3546818, 4.5346165e-05]\n","Batch:15\n","d_loss:0.16551706302198\n","g_loss:[0.34425652, 0.34416726, 4.462944e-05]\n","Batch:16\n","d_loss:0.16791915200883523\n","g_loss:[0.38375062, 0.3836624, 4.411077e-05]\n","Batch:17\n","d_loss:0.16468869542586617\n","g_loss:[2.6223376, 2.6222506, 4.3461692e-05]\n","Batch:18\n","d_loss:0.16362974290677812\n","g_loss:[0.34985813, 0.34977108, 4.352096e-05]\n","Batch:19\n","d_loss:0.16321260761469603\n","g_loss:[0.6106632, 0.6105768, 4.318226e-05]\n","Batch:20\n","d_loss:0.16492661701340694\n","g_loss:[0.3769424, 0.37685806, 4.2165524e-05]\n","Batch:21\n","d_loss:0.1629020932596177\n","g_loss:[0.36891392, 0.3688326, 4.0668674e-05]\n","Batch:22\n","d_loss:0.1651278529316187\n","g_loss:[0.36590347, 0.36582485, 3.931663e-05]\n","Batch:23\n","d_loss:0.1639481867532595\n","g_loss:[0.36261305, 0.36253622, 3.8413655e-05]\n","Batch:24\n","d_loss:0.16524898732313886\n","g_loss:[0.35230577, 0.35223028, 3.7742797e-05]\n","Batch:25\n","d_loss:0.16365405001852196\n","g_loss:[0.37077168, 0.37069738, 3.7144273e-05]\n","Batch:26\n","d_loss:0.1634492507982941\n","g_loss:[3.4587371, 3.458664, 3.655845e-05]\n","Batch:27\n","d_loss:0.16318541012878995\n","g_loss:[0.3593983, 0.35932636, 3.5968234e-05]\n","Batch:28\n","d_loss:0.16433986629999708\n","g_loss:[0.363237, 0.36316612, 3.5432786e-05]\n","Batch:29\n","d_loss:0.16510681773070246\n","g_loss:[0.37490997, 0.3748402, 3.4888806e-05]\n","Batch:30\n","d_loss:0.16960787028074265\n","g_loss:[0.33536115, 0.33529246, 3.434662e-05]\n","Batch:31\n","d_loss:0.1638499050095561\n","g_loss:[0.3383452, 0.33827764, 3.3782326e-05]\n","Batch:32\n","d_loss:0.16353927645104704\n","g_loss:[0.3397128, 0.3396463, 3.3251832e-05]\n","Batch:33\n","d_loss:0.16439576205448247\n","g_loss:[2.4399078, 2.4398425, 3.2715318e-05]\n","Batch:34\n","d_loss:0.26555120199918747\n","g_loss:[0.370082, 0.3700174, 3.228373e-05]\n","Batch:35\n","d_loss:0.1906862212345004\n","g_loss:[0.38363484, 0.38357067, 3.20801e-05]\n","Batch:36\n","d_loss:1.038182944059372\n","g_loss:[0.41924757, 0.41918468, 3.1448617e-05]\n","Batch:37\n","d_loss:0.31379112508147955\n","g_loss:[0.6014426, 0.6013813, 3.0637802e-05]\n"],"name":"stdout"},{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["========================================\n","Epoch is: 375\n","Number of batches:37\n","Batch:1\n","d_loss:3.301663190126419\n","g_loss:[2.4326308, 2.4325707, 3.0090789e-05]\n","Batch:2\n","d_loss:2.503634035587311\n","g_loss:[1.1202862, 1.120218, 3.412009e-05]\n","Batch:3\n","d_loss:1.483655035495758\n","g_loss:[2.362594, 2.3623228, 0.00013554066]\n","Batch:4\n","d_loss:1.977427363395691\n","g_loss:[3.5789752, 3.5783913, 0.000291896]\n","Batch:5\n","d_loss:1.5350624024868011\n","g_loss:[2.9111736, 2.9108613, 0.00015611073]\n","Batch:6\n","d_loss:0.6340857171453536\n","g_loss:[7.0484757, 7.048261, 0.00010728673]\n","Batch:7\n","d_loss:0.531386524438858\n","g_loss:[10.559972, 10.559793, 8.9223606e-05]\n","Batch:8\n","d_loss:0.8648100793361664\n","g_loss:[7.951413, 7.951076, 0.00016850642]\n","Batch:9\n","d_loss:0.47315138578414917\n","g_loss:[6.3540273, 6.353676, 0.00017567484]\n","Batch:10\n","d_loss:0.44996051490306854\n","g_loss:[7.071651, 7.0713167, 0.0001672201]\n","Batch:11\n","d_loss:0.8327319324016571\n","g_loss:[7.84013, 7.839649, 0.00024043182]\n","Batch:12\n","d_loss:1.1753108203411102\n","g_loss:[0.9974139, 0.9968907, 0.00026158572]\n","Batch:13\n","d_loss:2.401401787996292\n","g_loss:[4.699567, 4.699084, 0.00024144002]\n","Batch:14\n","d_loss:1.578095555305481\n","g_loss:[5.0965166, 5.095252, 0.0006323432]\n","Batch:15\n","d_loss:0.45735274255275726\n","g_loss:[9.713854, 9.711677, 0.0010887411]\n","Batch:16\n","d_loss:2.235874891281128\n","g_loss:[4.924997, 4.921864, 0.0015665276]\n","Batch:17\n","d_loss:0.4467208832502365\n","g_loss:[2.5055141, 2.5017955, 0.0018593146]\n","Batch:18\n","d_loss:0.4528606981039047\n","g_loss:[7.86425, 7.858863, 0.0026936817]\n","Batch:19\n","d_loss:1.2719182223081589\n","g_loss:[3.5022967, 3.5000997, 0.0010985571]\n","Batch:20\n","d_loss:0.8947897255420685\n","g_loss:[3.0559256, 3.0539777, 0.00097391574]\n","Batch:21\n","d_loss:0.40674399584531784\n","g_loss:[3.0274274, 3.0251288, 0.0011492558]\n","Batch:22\n","d_loss:2.550265908241272\n","g_loss:[2.0366297, 2.034308, 0.0011608468]\n","Batch:23\n","d_loss:0.5154858082532883\n","g_loss:[1.7492268, 1.7475547, 0.000836077]\n","Batch:24\n","d_loss:0.35801907628774643\n","g_loss:[5.796669, 5.79544, 0.0006143765]\n","Batch:25\n","d_loss:0.4012566804885864\n","g_loss:[3.4430954, 3.4418976, 0.0005988779]\n","Batch:26\n","d_loss:0.48260973393917084\n","g_loss:[3.934895, 3.9335582, 0.00066839997]\n","Batch:27\n","d_loss:0.4962005615234375\n","g_loss:[5.227788, 5.225692, 0.0010481165]\n","Batch:28\n","d_loss:0.9657919704914093\n","g_loss:[4.7446175, 4.742218, 0.0011996379]\n","Batch:29\n","d_loss:0.533734530210495\n","g_loss:[3.4785116, 3.4762444, 0.0011335539]\n","Batch:30\n","d_loss:0.47066739201545715\n","g_loss:[4.846908, 4.8448915, 0.0010082448]\n","Batch:31\n","d_loss:0.6631875783205032\n","g_loss:[6.2694607, 6.2677107, 0.00087488093]\n","Batch:32\n","d_loss:0.5195444747805595\n","g_loss:[9.32489, 9.323353, 0.0007688645]\n","Batch:33\n","d_loss:0.5297356098890305\n","g_loss:[7.88925, 7.8877907, 0.00072967284]\n","Batch:34\n","d_loss:1.1455390900373459\n","g_loss:[5.4933662, 5.4918847, 0.00074081204]\n","Batch:35\n","d_loss:0.417679563164711\n","g_loss:[2.385482, 2.383832, 0.0008250899]\n","Batch:36\n","d_loss:0.6191688291728497\n","g_loss:[2.4095445, 2.4078012, 0.00087161106]\n","Batch:37\n","d_loss:0.575202077627182\n","g_loss:[6.7011395, 6.699638, 0.00075067685]\n","========================================\n","Epoch is: 376\n","Number of batches:37\n","Batch:1\n","d_loss:2.169531062245369\n","g_loss:[4.847893, 4.846632, 0.0006306836]\n","Batch:2\n","d_loss:1.329567551612854\n","g_loss:[4.1948924, 4.1937423, 0.0005751569]\n","Batch:3\n","d_loss:1.1330306231975555\n","g_loss:[1.16233, 1.1611774, 0.0005763203]\n","Batch:4\n","d_loss:0.39954476058483124\n","g_loss:[1.6240631, 1.6229542, 0.000554448]\n","Batch:5\n","d_loss:0.4855048358440399\n","g_loss:[4.4000235, 4.3990693, 0.00047695742]\n","Batch:6\n","d_loss:0.3760467544198036\n","g_loss:[5.4404564, 5.439644, 0.00040629192]\n","Batch:7\n","d_loss:0.46257342398166656\n","g_loss:[7.2010646, 7.2003355, 0.0003645776]\n","Batch:8\n","d_loss:0.5018753856420517\n","g_loss:[3.7879052, 3.7872171, 0.00034407724]\n","Batch:9\n","d_loss:0.5648526027798653\n","g_loss:[6.100852, 6.1002054, 0.0003233981]\n","Batch:10\n","d_loss:0.5224302709102631\n","g_loss:[6.6597185, 6.659093, 0.00031269505]\n","Batch:11\n","d_loss:0.5357754677534103\n","g_loss:[2.9327552, 2.9321635, 0.00029582728]\n","Batch:12\n","d_loss:0.5668835490942001\n","g_loss:[1.2737643, 1.2732012, 0.00028151134]\n","Batch:13\n","d_loss:0.5125271379947662\n","g_loss:[3.6341143, 3.6335628, 0.00027574998]\n","Batch:14\n","d_loss:2.490656614303589\n","g_loss:[1.0755266, 1.0750158, 0.0002553855]\n","Batch:15\n","d_loss:0.4538164436817169\n","g_loss:[3.9002466, 3.899768, 0.00023924503]\n","Batch:16\n","d_loss:1.8686506748199463\n","g_loss:[2.5806835, 2.5802398, 0.00022189977]\n","Batch:17\n","d_loss:0.5384553372859955\n","g_loss:[3.060101, 3.0596855, 0.00020779826]\n","Batch:18\n","d_loss:0.49891242384910583\n","g_loss:[3.8040426, 3.8036377, 0.0002023925]\n","Batch:19\n","d_loss:0.48687200248241425\n","g_loss:[3.7191334, 3.7187364, 0.0001984758]\n","Batch:20\n","d_loss:0.514278270304203\n","g_loss:[2.4313486, 2.4309652, 0.00019163128]\n","Batch:21\n","d_loss:0.49233323335647583\n","g_loss:[3.5029948, 3.502616, 0.00018942863]\n","Batch:22\n","d_loss:0.6870625168085098\n","g_loss:[2.6908143, 2.6904628, 0.00017570292]\n","Batch:23\n","d_loss:0.5411864966154099\n","g_loss:[1.6183053, 1.6179702, 0.00016753221]\n","Batch:24\n","d_loss:0.32285432144999504\n","g_loss:[3.034348, 3.0340166, 0.00016568924]\n","Batch:25\n","d_loss:0.45461370795965195\n","g_loss:[2.5861764, 2.585843, 0.00016668241]\n","Batch:26\n","d_loss:0.5843982249498367\n","g_loss:[3.938113, 3.9377875, 0.00016273414]\n","Batch:27\n","d_loss:0.47096505761146545\n","g_loss:[2.6499653, 2.6496484, 0.0001584109]\n","Batch:28\n","d_loss:2.351043850183487\n","g_loss:[0.93493044, 0.9346156, 0.00015742339]\n","Batch:29\n","d_loss:0.5840683877468109\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hf_xM9WHJUXq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}